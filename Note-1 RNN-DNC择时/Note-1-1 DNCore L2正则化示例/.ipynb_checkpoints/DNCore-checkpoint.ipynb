{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNCore\n",
    "\n",
    "#### Hybrid computing using a neural network with dynamic external memory\n",
    "#### differentiable neural computer\n",
    "\n",
    "##### 2017-09-02 DNCore封装\n",
    "- calculate 该模块不参与时序间传递\n",
    "- update 该模块参与时序间传递，需要上一时刻模块状态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sonnet.python.modules.base import AbstractModule\n",
    "from sonnet.python.modules.basic import BatchApply, Linear, BatchFlatten\n",
    "from sonnet.python.modules.rnn_core import RNNCore\n",
    "from sonnet.python.modules.gated_rnn import LSTM\n",
    "from sonnet.python.modules.basic_rnn import DeepRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access\n",
    "### Memory Addressing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于内容寻址机制 Content-based addressing\n",
    "- 使用余弦相似性处理外存储器Access 记忆矩阵中数值相似性\n",
    "- 读头控制和写头控制寻址机制组成模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Content-based addressing\n",
    "class calculate_Content_based_addressing(AbstractModule):\n",
    "    \"\"\"\n",
    "    查询计算记忆矩阵每行内容记忆之间的余弦相似度，\n",
    "    使用softmax返回一个数值大小嵌入[0,1]区间tensor。\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                num_heads, \n",
    "                word_size,\n",
    "                epsilon = 1e-6,\n",
    "                name='content_based_addressing'):\n",
    "        \"\"\"\n",
    "        Initializes the module.\n",
    "\n",
    "        Args:\n",
    "          num_heads: number of memory write heads or read heads.\n",
    "          word_size: memory word size.\n",
    "          epsilon: 鲁棒性功能添加\n",
    "          name: module name (default 'content_based_addressing')\n",
    "        \"\"\"\n",
    "        super().__init__(name=name) # 调用父类初始化\n",
    "        self._num_heads = num_heads\n",
    "        self._word_size = word_size\n",
    "        self._epsilon = epsilon\n",
    "        \n",
    "\n",
    "    def _clip_L2_norm(self, tensor, axis=2):\n",
    "        \"\"\"\n",
    "        计算L2范数，余弦相似度公式分母，\n",
    "        这里进行数值平稳化处理\n",
    "        memory: A 3-D tensor of shape [batch_size, memory_size, word_size]\n",
    "        keys: A 3-D tensor of shape [batch_size, num_heads, word_size]  \n",
    "        \"\"\"\n",
    "        quadratic_sum = tf.reduce_sum(tf.multiply(tensor, tensor), axis=axis, keep_dims=True)    \n",
    "        return tf.sqrt(quadratic_sum + self._epsilon)\n",
    "    \n",
    "    \n",
    "    def _calculate_cosine_similarity(self, keys, memory):\n",
    "        \n",
    "        \"\"\"\n",
    "        计算余弦相似度\n",
    "        Args:      \n",
    "            memory: A 3-D tensor of shape [batch_size, memory_size, word_size]\n",
    "            keys: A 3-D tensor of shape [batch_size, num_heads, word_size]  \n",
    "        Returns:\n",
    "            cosine_similarity: A 3-D tensor of shape `[batch_size, num_heads, memory_size]`.\n",
    "        \"\"\"\n",
    "        # 分子\n",
    "        matmul = tf.matmul(keys, memory, adjoint_b=True)\n",
    "        # 分母\n",
    "        memory_norm = self._clip_L2_norm(memory, axis=2)\n",
    "        keys_norm = self._clip_L2_norm(keys, axis=2)\n",
    "        # 余弦相似度计算， 添加epsilon消除极值影响；\n",
    "        cosine_similarity = matmul / (tf.matmul(keys_norm, memory_norm, adjoint_b=True) + self._epsilon)\n",
    "        return cosine_similarity\n",
    "    \n",
    "    \n",
    "    def _build(self, memory, keys, strengths):\n",
    "        \"\"\"\n",
    "        Connects the CosineWeights module into the graph.\n",
    "        计算余弦相似度\n",
    "        使用write strength或者read strength 适度缩放余弦相似度。\n",
    "        提高不同读写头的读头控制、写头控制区分度。\n",
    "\n",
    "        Args:\n",
    "            memory: A 3-D tensor of shape `[batch_size, memory_size, word_size]`.\n",
    "            keys: A 3-D tensor of shape `[batch_size, num_heads, word_size]`.\n",
    "            strengths: A 2-D tensor of shape `[batch_size, num_heads]`.\n",
    "\n",
    "        Returns:\n",
    "            cosine_similarity: A 3-D tensor of shape `[batch_size, num_heads, memory_size]`.\n",
    "            content_weighting: Weights tensor of shape `[batch_size, num_heads, memory_size]`.\n",
    "        \"\"\"    \n",
    "        cosine_similarity = self._calculate_cosine_similarity(keys=keys, memory=memory)\n",
    "        transformed_strengths = tf.expand_dims(strengths, axis=-1)\n",
    "        sharp_activations = cosine_similarity * transformed_strengths\n",
    "        softmax = BatchApply(module_or_op=tf.nn.softmax)\n",
    "        return softmax(sharp_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 动态内存允许 Dynamic memory allocation\n",
    "- To allow the controller to free and allocate memory as needed, \n",
    "- we developed a differentiable analogue of the ‘free list’ memory allocation scheme,\n",
    "- whereby a list of available memory locations is maintained \n",
    "- by adding to and removing addresses from a linked list. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Dynamic_memory_allocation\n",
    "class update_Dynamic_memory_allocation(RNNCore):\n",
    "    \"\"\"\n",
    "    Memory usage that is increased by writing and decreased by reading.\n",
    "\n",
    "    This module is a pseudo-RNNCore whose state is a tensor with values in\n",
    "    the range [0, 1] indicating the usage of each of `memory_size` memory slots.\n",
    "\n",
    "    The usage is:\n",
    "\n",
    "    *   Increased by writing, where usage is increased towards 1 at the write\n",
    "      addresses.\n",
    "    *   Decreased by reading, where usage is decreased after reading from a\n",
    "      location when free_gates is close to 1.\n",
    "    \"\"\"  \n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        memory_size,\n",
    "        epsilon = 1e-6,\n",
    "        name='dynamic_memory_allocation'):\n",
    "        \n",
    "        \"\"\"Creates a module for dynamic memory allocation.\n",
    "\n",
    "        Args:\n",
    "          memory_size: Number of memory slots.\n",
    "          name: Name of the module.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self._memory_size = memory_size\n",
    "        self._epsilon = epsilon\n",
    "    \n",
    "    \n",
    "    def _build(\n",
    "        self,\n",
    "        prev_usage,\n",
    "        prev_write_weightings,\n",
    "        free_gates,\n",
    "        prev_read_weightings,\n",
    "        write_gates, \n",
    "        num_writes):\n",
    "        \n",
    "        # 更新记忆矩阵每行的使用程度，区间[0,1]\n",
    "        # 程度数值随写入行为提高，读取行为降低\n",
    "        usage = self._update_usage_vector(\n",
    "            prev_usage, \n",
    "            prev_write_weightings,\n",
    "            free_gates, \n",
    "            prev_read_weightings)\n",
    "        \n",
    "        # 记忆矩阵行位置释放\n",
    "        allocation_weightings = self._update_allocation_weightings(\n",
    "            usage, write_gates, num_writes)\n",
    "        \n",
    "        return usage, allocation_weightings\n",
    "    \n",
    "    \n",
    "    def _update_usage_vector(\n",
    "        self, \n",
    "        prev_usage,\n",
    "        prev_write_weightings,\n",
    "        free_gates, \n",
    "        prev_read_weightings):\n",
    "        \"\"\"\n",
    "        The usage is:\n",
    "\n",
    "        *   Increased by writing, where usage is increased towards 1 at the write\n",
    "          addresses.\n",
    "        *   Decreased by reading, where usage is decreased after reading from a\n",
    "          location when free_gates is close to 1.\n",
    "        \n",
    "        Args:\n",
    "            prev_usage: tensor of shape `[batch_size, memory_size]` giving\n",
    "            usage u_{t - 1} at the previous time step, with entries in range [0, 1].\n",
    "        \n",
    "            prev_write_weightings: tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "            giving write weights at previous time step.\n",
    "            \n",
    "            free_gates: tensor of shape `[batch_size, num_reads]` which indicates\n",
    "            which read heads read memory that can now be freed.\n",
    "          \n",
    "            prev_read_weightings: tensor of shape `[batch_size, num_reads, memory_size]` \n",
    "            giving read weights at previous time step.\n",
    "          \n",
    "        Returns:\n",
    "            usage: tensor of shape `[batch_size, memory_size]` representing updated memory usage.\n",
    "        \"\"\"\n",
    "        prev_write_weightings = tf.stop_gradient(prev_write_weightings)\n",
    "        usage = self._calculate_usage_vector(prev_usage, prev_write_weightings)\n",
    "        retention = self._calculate_retention_vector(free_gates, prev_read_weightings)\n",
    "        return usage * retention\n",
    "    \n",
    "    \n",
    "    def _calculate_usage_vector(\n",
    "        self, \n",
    "        prev_usage, \n",
    "        prev_write_weightings):\n",
    "        \"\"\"\n",
    "        注意这里usage更新使用上一个时间步的数据\n",
    "        这个函数是特别添加处理多个写头写头控制情况,\n",
    "        这个函数计算在写头操作之后记忆矩阵的使用情况usage\n",
    "        \n",
    "        Calcualtes the new usage after writing to memory.\n",
    "\n",
    "        Args:\n",
    "          prev_usage: tensor of shape `[batch_size, memory_size]`.\n",
    "          write_weightings: tensor of shape `[batch_size, num_writes, memory_size]`.\n",
    "\n",
    "        Returns:\n",
    "          New usage, a tensor of shape `[batch_size, memory_size]`.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('usage_after_write'):\n",
    "            # Calculate the aggregated effect of all write heads\n",
    "            fit_prev_write_weightings = \\\n",
    "            1 - tf.reduce_prod(1 - prev_write_weightings, axis=[1])\n",
    "            \n",
    "            usage_without_free = \\\n",
    "            prev_usage + fit_prev_write_weightings - prev_usage * fit_prev_write_weightings\n",
    "            \n",
    "            return usage_without_free\n",
    "        \n",
    "        \n",
    "    def _calculate_retention_vector(\n",
    "        self, \n",
    "        free_gates, \n",
    "        prev_read_weightings):\n",
    "        \n",
    "        \"\"\"\n",
    "        The memory retention vector phi_t represents by how much each location \n",
    "        will not be freed by the gates.\n",
    "        \n",
    "        Args:\n",
    "            free_gates: tensor of shape `[batch_size, num_reads]` with entries in the\n",
    "            range [0, 1] indicating the amount that locations read from can be\n",
    "            freed.\n",
    "            \n",
    "            prev_write_weightings: tensor of shape `[batch_size, num_writes, memory_size]`.\n",
    "        Returns:\n",
    "            retention vector: [batch_size, memory_size]\n",
    "        \"\"\"\n",
    "        with tf.name_scope('usage_after_read'):\n",
    "            free_gates = tf.expand_dims(free_gates, axis=-1)\n",
    "            \n",
    "            retention_vector = tf.reduce_prod(\n",
    "                1 - free_gates * prev_read_weightings, \n",
    "                axis=[1], name='retention')\n",
    "            return retention_vector     \n",
    "        \n",
    "        \n",
    "    def _update_allocation_weightings(\n",
    "        self, \n",
    "        usage, \n",
    "        write_gates, \n",
    "        num_writes):\n",
    "        \n",
    "        \"\"\"\n",
    "        Calculates freeness-based locations for writing to.\n",
    "\n",
    "        This finds unused memory by ranking the memory locations by usage, for each\n",
    "        write head. (For more than one write head, we use a \"simulated new usage\"\n",
    "        which takes into account the fact that the previous write head will increase\n",
    "        the usage in that area of the memory.)\n",
    "\n",
    "        Args:\n",
    "            usage: A tensor of shape `[batch_size, memory_size]` representing\n",
    "            current memory usage.\n",
    "\n",
    "            write_gates: A tensor of shape `[batch_size, num_writes]` with values in\n",
    "            the range [0, 1] indicating how much each write head does writing\n",
    "            based on the address returned here (and hence how much usage\n",
    "            increases).\n",
    "\n",
    "            num_writes: The number of write heads to calculate write weights for.\n",
    "\n",
    "        Returns:\n",
    "            tensor of shape `[batch_size, num_writes, memory_size]` containing the\n",
    "            freeness-based write locations. Note that this isn't scaled by `write_gate`; \n",
    "            this scaling must be applied externally.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('update_allocation'):\n",
    "            write_gates = tf.expand_dims(write_gates, axis=-1)\n",
    "            allocation_weightings = []\n",
    "            for i in range(num_writes):\n",
    "                allocation_weightings.append(\n",
    "                    self._calculate_allocation_weighting(usage))\n",
    "                # update usage to take into account writing to this new allocation\n",
    "                usage += ((1-usage) * write_gates[:,i,:] * allocation_weightings[i])\n",
    "            return tf.stack(allocation_weightings, axis=1)\n",
    "        \n",
    "        \n",
    "    def _calculate_allocation_weighting(self, usage):\n",
    "        \n",
    "        \"\"\"\n",
    "        Computes allocation by sorting `usage`.\n",
    "\n",
    "        This corresponds to the value a = a_t[\\phi_t[j]] in the paper.\n",
    "\n",
    "        Args:\n",
    "              usage: tensor of shape `[batch_size, memory_size]` indicating current\n",
    "              memory usage. This is equal to u_t in the paper when we only have one\n",
    "              write head, but for multiple write heads, one should update the usage\n",
    "              while iterating through the write heads to take into account the\n",
    "              allocation returned by this function.\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape `[batch_size, memory_size]` corresponding to allocation.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('allocation'):\n",
    "            # Ensure values are not too small prior to cumprod.\n",
    "            usage = self._epsilon + (1 - self._epsilon) * usage\n",
    "            non_usage = 1 - usage\n",
    "            \n",
    "            sorted_non_usage, indices = tf.nn.top_k(\n",
    "            non_usage, k = self._memory_size, name='sort')\n",
    "            \n",
    "            sorted_usage = 1 - sorted_non_usage\n",
    "            prod_sorted_usage = tf.cumprod(sorted_usage, axis=1, exclusive=True)\n",
    "            \n",
    "            sorted_allocation_weighting = sorted_non_usage * prod_sorted_usage\n",
    "            \n",
    "            # This final line \"unsorts\" sorted_allocation, so that the indexing\n",
    "            # corresponds to the original indexing of `usage`.\n",
    "            inverse_indices = self._batch_invert_permutation(indices)\n",
    "            allocation_weighting = self._batch_gather(\n",
    "                sorted_allocation_weighting, inverse_indices)\n",
    "            \n",
    "            return allocation_weighting\n",
    "            \n",
    "\n",
    "    def _batch_invert_permutation(self, permutations):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns batched `tf.invert_permutation` for every row in `permutations`.\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.name_scope('batch_invert_permutation', values=[permutations]):\n",
    "            unpacked = tf.unstack(permutations, axis=0)\n",
    "            \n",
    "            inverses = [tf.invert_permutation(permutation) for permutation in unpacked]\n",
    "            return tf.stack(inverses, axis=0)\n",
    "        \n",
    "              \n",
    "    def _batch_gather(self, values, indices):\n",
    "        \"\"\"Returns batched `tf.gather` for every row in the input.\"\"\"\n",
    "        \n",
    "        with tf.name_scope('batch_gather', values=[values, indices]):\n",
    "            unpacked = zip(tf.unstack(values), tf.unstack(indices))\n",
    "            result = [tf.gather(value, index) for value, index in unpacked]\n",
    "            return tf.stack(result)   \n",
    "        \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于读写行为顺序头控制 Temporal memory linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Temporal_memory_linkage\n",
    "class update_Temporal_memory_linkage(RNNCore):\n",
    "    \"\"\"\n",
    "    Keeps track of write order for forward and backward addressing.\n",
    "\n",
    "    This is a pseudo-RNNCore module, whose state is a pair `(link,\n",
    "    precedence_weights)`, where `link` is a (collection of) graphs for (possibly\n",
    "    multiple) write heads (represented by a tensor with values in the range\n",
    "    [0, 1]), and `precedence_weights` records the \"previous write locations\" used\n",
    "    to build the link graphs.\n",
    "\n",
    "    The function `directional_read_weights` computes addresses following the\n",
    "    forward and backward directions in the link graphs.\n",
    "    \"\"\" \n",
    "    def __init__(self, \n",
    "                 memory_size, \n",
    "                 num_writes, \n",
    "                 name='temporal_memory_linkage'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Construct a TemporalLinkage module.\n",
    "\n",
    "        Args:\n",
    "          memory_size: The number of memory slots.\n",
    "          num_writes: The number of write heads.\n",
    "          name: Name of the module.\n",
    "        \"\"\"  \n",
    "        super().__init__(name=name)\n",
    "        self._memory_size = memory_size\n",
    "        self._num_writes = num_writes\n",
    "        \n",
    "        \n",
    "    def _build(self, \n",
    "               prev_link, \n",
    "               prev_precedence_weightings,\n",
    "               prev_read_weightings,\n",
    "               write_weightings):\n",
    "        \"\"\"\n",
    "        calculate the updated linkage state given the write weights.\n",
    "        \n",
    "        Args:           \n",
    "            prev_links: A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n",
    "            representing the previous link graphs for each write head.\n",
    "\n",
    "            prev_precedence_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "            containing the previous precedence weights.\n",
    "\n",
    "            write_weightings: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "            containing the memory addresses of the different write heads.\n",
    "        \n",
    "        Returns:\n",
    "            link:  A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n",
    "            precedence_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "\n",
    "        \"\"\"\n",
    "        link = self._update_link_matrix(\n",
    "            prev_link, prev_precedence_weightings, write_weightings)\n",
    "        \n",
    "        precedence_weightings = \\\n",
    "        self._update_precedence_weightings(\n",
    "            prev_precedence_weightings, write_weightings)\n",
    "        \n",
    "        forward_weightings = \\\n",
    "        self._calculate_directional_read_weightings(\n",
    "            link, prev_read_weightings, forward=True)\n",
    "        \n",
    "        backward_weightings = \\\n",
    "        self._calculate_directional_read_weightings(\n",
    "            link, prev_read_weightings, forward=False)\n",
    "        \n",
    "        return link, precedence_weightings, forward_weightings, backward_weightings  \n",
    "    \n",
    "    \n",
    "    def _update_link_matrix(self, \n",
    "                            prev_link, \n",
    "                            prev_precedence_weightings, \n",
    "                            write_weightings):\n",
    "        \"\"\"\n",
    "        calculates the new link graphs.\n",
    "\n",
    "        For each write head, the link is a directed graph (represented by a matrix\n",
    "        with entries in range [0, 1]) whose vertices are the memory locations, and\n",
    "        an edge indicates temporal ordering of writes.\n",
    "\n",
    "        Args:\n",
    "          prev_links: A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n",
    "          representing the previous link graphs for each write head.\n",
    "\n",
    "          prev_precedence_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "          which is the previous \"aggregated\" write weights for each write head.\n",
    "\n",
    "          write_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "              containing the new locations in memory written to.\n",
    "\n",
    "        Returns:\n",
    "          A tensor of shape `[batch_size, num_writes, memory_size, memory_size]`\n",
    "          containing the new link graphs for each write head.\n",
    "        \"\"\"    \n",
    "        \n",
    "        with tf.name_scope('link'):\n",
    "                   \n",
    "            write_weightings_i = tf.expand_dims(write_weightings, axis=3)\n",
    "            write_weightings_j = tf.expand_dims(write_weightings, axis=2)\n",
    "            prev_link_scale = 1 - write_weightings_i - write_weightings_j\n",
    "            remove_old_link = prev_link_scale * prev_link\n",
    "            \n",
    "            prev_precedence_weightings_j = tf.expand_dims(\n",
    "                prev_precedence_weightings, axis=2)\n",
    "            add_new_link = write_weightings_i * prev_precedence_weightings_j\n",
    "            \n",
    "            link = remove_old_link + add_new_link\n",
    "            \n",
    "            #Return the link with the diagonal set to zero, to remove self-looping edges.\n",
    "            batch_size = prev_link.get_shape()[0].value\n",
    "            mask = tf.zeros(shape=[batch_size, self._num_writes, self._memory_size], \n",
    "                            dtype=prev_link.dtype)\n",
    "            \n",
    "            fit_link = tf.matrix_set_diag(link, diagonal=mask)\n",
    "            return fit_link\n",
    "        \n",
    "        \n",
    "    def _update_precedence_weightings(self, \n",
    "                                     prev_precedence_weightings, \n",
    "                                     write_weightings):\n",
    "        \"\"\"\n",
    "        calculates the new precedence weights given the current write weights.\n",
    "\n",
    "        The precedence weights are the \"aggregated write weights\" for each write\n",
    "        head, where write weights with sum close to zero will leave the precedence\n",
    "        weights unchanged, but with sum close to one will replace the precedence\n",
    "        weights.   \n",
    "\n",
    "        Args:\n",
    "          prev_precedence_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "          containing the previous precedence weights.\n",
    "\n",
    "          write_weightings: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "          containing the new write weights.\n",
    "\n",
    "        Returns:\n",
    "          A tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "          containing the new precedence weights.  \n",
    "        \"\"\"\n",
    "        with tf.name_scope('precedence_weightings'):\n",
    "            sum_writing = tf.reduce_sum(write_weightings, axis=2, keep_dims=True)\n",
    "            \n",
    "            precedence_weightings = \\\n",
    "            (1 - sum_writing) * prev_precedence_weightings + write_weightings\n",
    "            \n",
    "            return precedence_weightings\n",
    "        \n",
    "\n",
    "    def _calculate_directional_read_weightings(self,\n",
    "                                               link, \n",
    "                                               prev_read_weightings, \n",
    "                                               forward):\n",
    "        \"\"\"\n",
    "        calculates the forward or the backward read weightings.\n",
    "\n",
    "        For each read head (at a given address), there are `num_writes` link graphs to follow. \n",
    "        Thus this function computes a read address for each of the\n",
    "        `num_reads * num_writes` pairs of read and write heads.\n",
    "\n",
    "        Args:\n",
    "            link: tensor of shape `[batch_size, num_writes, memory_size, memory_size]`\n",
    "            representing the link graphs L_t.\n",
    "\n",
    "            prev_read_weightsing: tensor of shape `[batch_size, num_reads, memory_size]` \n",
    "            containing the previous read weights w_{t-1}^r.\n",
    "\n",
    "            forward: Boolean indicating whether to follow the \"future\" direction in \n",
    "            the link graph (True) or the \"past\" direction (False).\n",
    "\n",
    "        Returns:\n",
    "            tensor of shape `[batch_size, num_reads, num_writes, memory_size]`\n",
    "\n",
    "            Note: We calculate the forward and backward directions for each pair of\n",
    "            read and write heads; hence we need to tile the read weights and do a\n",
    "            sort of \"outer product\" to get this.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('directional_read_weightings'):\n",
    "            # We calculate the forward and backward directions for each pair of\n",
    "            # read and write heads; hence we need to tile the read weights and do a\n",
    "            # sort of \"outer product\" to get this.\n",
    "            expanded_read_weightings = \\\n",
    "            tf.stack([prev_read_weightings] * self._num_writes, axis=1)\n",
    "            directional_weightings = tf.matmul(expanded_read_weightings, link, adjoint_b=forward)\n",
    "            # Swap dimensions 1, 2 so order is [batch, reads, writes, memory]:\n",
    "            return tf.transpose(directional_weightings, perm=[0,2,1,3])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MemoryAccess\n",
    "class MemoryAccess(RNNCore):\n",
    "    \"\"\"\n",
    "    Access module of the Differentiable Neural Computer.\n",
    "\n",
    "    This memory module supports multiple read and write heads. It makes use of:\n",
    "\n",
    "    *   `update_Temporal_memory_linkage` to track the temporal \n",
    "    ordering of writes in memory for each write head.\n",
    "    \n",
    "    *   `update_Dynamic_memory_allocation` for keeping track of \n",
    "    memory usage, where usage increase when a memory location is \n",
    "    written to, and decreases when memory is read from that \n",
    "    the controller says can be freed.\n",
    "      \n",
    "    Write-address selection is done by an interpolation between content-based\n",
    "    lookup and using unused memory.\n",
    "    \n",
    "    Read-address selection is done by an interpolation of content-based lookup\n",
    "    and following the link graph in the forward or backwards read direction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 memory_size = 128, \n",
    "                 word_size = 20, \n",
    "                 num_reads = 1, \n",
    "                 num_writes = 1, \n",
    "                 name='memory_access'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a MemoryAccess module.\n",
    "\n",
    "        Args:\n",
    "            memory_size: The number of memory slots (N in the DNC paper).\n",
    "            word_size: The width of each memory slot (W in the DNC paper)\n",
    "            num_reads: The number of read heads (R in the DNC paper).\n",
    "            num_writes: The number of write heads (fixed at 1 in the paper).\n",
    "            name: The name of the module.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self._memory_size = memory_size\n",
    "        self._word_size = word_size\n",
    "        self._num_reads = num_reads\n",
    "        self._num_writes = num_writes\n",
    "        \n",
    "        self._write_content_mod = calculate_Content_based_addressing(\n",
    "            num_heads = self._num_writes, \n",
    "            word_size = self._word_size, \n",
    "            name = 'write_content_based_addressing')\n",
    "        \n",
    "        self._read_content_mod = calculate_Content_based_addressing(\n",
    "            num_heads = self._num_reads, \n",
    "            word_size = self._word_size, \n",
    "            name = 'read_content_based_addressing')\n",
    "        \n",
    "        self._temporal_linkage = update_Temporal_memory_linkage(\n",
    "            memory_size = self._memory_size, \n",
    "            num_writes = self._num_writes)\n",
    "        \n",
    "        self._dynamic_allocation = update_Dynamic_memory_allocation(\n",
    "            memory_size = self._memory_size)\n",
    "        \n",
    "        \n",
    "    def _build(self, interface_vector, prev_state):\n",
    "        \"\"\"\n",
    "        Connects the MemoryAccess module into the graph.\n",
    "\n",
    "        Args:\n",
    "            inputs: tensor of shape `[batch_size, input_size]`. \n",
    "            This is used to control this access module.\n",
    "            \n",
    "            prev_state: Instance of `AccessState` containing the previous state.\n",
    "\n",
    "        Returns:\n",
    "            A tuple `(output, next_state)`, where `output` is a tensor of shape\n",
    "            `[batch_size, num_reads, word_size]`, and `next_state` is the new\n",
    "            `AccessState` named tuple at the current time t.\n",
    "        \"\"\"\n",
    "        tape = self._Calculate_interface_parameters(interface_vector)\n",
    "        \n",
    "        prev_memory,\\\n",
    "        prev_read_weightings,\\\n",
    "        prev_write_weightings,\\\n",
    "        prev_precedence_weightings,\\\n",
    "        prev_link,\\\n",
    "        prev_usage = prev_state\n",
    "\n",
    "        # 更新写头\n",
    "        write_weightings,\\\n",
    "        usage = \\\n",
    "        self._update_write_weightings(tape, \n",
    "                                      prev_memory, \n",
    "                                      prev_usage, \n",
    "                                      prev_write_weightings, \n",
    "                                      prev_read_weightings)\n",
    "        \n",
    "        # 更新记忆\n",
    "        memory = self._update_memory(prev_memory, \n",
    "                                     write_weightings, \n",
    "                                     tape['erase_vectors'], \n",
    "                                     tape['write_vectors'])\n",
    "        \n",
    "        # 更新读头\n",
    "        read_weightings,\\\n",
    "        link,\\\n",
    "        precedence_weightings= \\\n",
    "        self._update_read_weightings(tape, \n",
    "                                     memory, \n",
    "                                     write_weightings,\n",
    "                                     prev_read_weightings, \n",
    "                                     prev_precedence_weightings,\n",
    "                                     prev_link)\n",
    "\n",
    "        read_vectors = tf.matmul(read_weightings, memory)\n",
    "        \n",
    "        state = (memory,\n",
    "                 read_weightings,\n",
    "                 write_weightings,\n",
    "                 precedence_weightings,\n",
    "                 link,\n",
    "                 usage)\n",
    "        \n",
    "        return read_vectors, state\n",
    "        \n",
    "\n",
    "    def _update_write_weightings(self, \n",
    "                                  tape, \n",
    "                                  prev_memory, \n",
    "                                  prev_usage, \n",
    "                                  prev_write_weightings, \n",
    "                                  prev_read_weightings):       \n",
    "        \"\"\"\n",
    "        Calculates the memory locations to write to.\n",
    "\n",
    "        This uses a combination of content-based lookup and finding an unused\n",
    "        location in memory, for each write head.\n",
    "\n",
    "        Args:\n",
    "            tape: Collection of inputs to the access module, including controls for\n",
    "            how to chose memory writing, such as the content to look-up and the\n",
    "            weighting between content-based and allocation-based addressing.\n",
    "            \n",
    "            memory: A tensor of shape  `[batch_size, memory_size, word_size]`\n",
    "            containing the current memory contents.\n",
    "            \n",
    "            usage: Current memory usage, which is a tensor of shape \n",
    "            `[batch_size, memory_size]`, used for allocation-based addressing.\n",
    "\n",
    "        Returns:\n",
    "            tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "            indicating where to write to (if anywhere) for each write head.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('update_write_weightings', \\\n",
    "                           values=[tape, prev_memory, prev_usage]):\n",
    "            \n",
    "            write_content_weightings = \\\n",
    "            self._write_content_mod(\n",
    "                prev_memory, \n",
    "                tape['write_content_keys'], \n",
    "                tape['write_content_strengths'])\n",
    "            \n",
    "            usage, write_allocation_weightings = \\\n",
    "            self._dynamic_allocation(\n",
    "                prev_usage, \n",
    "                prev_write_weightings, \n",
    "                tape['free_gates'], \n",
    "                prev_read_weightings, \n",
    "                tape['write_gates'], \n",
    "                self._num_writes)\n",
    "            \n",
    "            allocation_gates = tf.expand_dims(tape['allocation_gates'], axis=-1)\n",
    "            write_gates = tf.expand_dims(tape['write_gates'], axis=-1)\n",
    "            \n",
    "            write_weightings = write_gates * \\\n",
    "            (allocation_gates * write_allocation_weightings + \\\n",
    "             (1 - allocation_gates) * write_content_weightings)\n",
    "            \n",
    "            return write_weightings, usage\n",
    "        \n",
    "        \n",
    "    def _update_memory(self, \n",
    "                       prev_memory, \n",
    "                       write_weightings, \n",
    "                       erase_vectors, \n",
    "                       write_vectors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prev_memory: 3-D tensor of shape `[batch_size, memory_size, word_size]`.\n",
    "            write_weightings: 3-D tensor `[batch_size, num_writes, memory_size]`.\n",
    "            erase_vectors: 3-D tensor `[batch_size, num_writes, word_size]`.\n",
    "            write_vectors: 3-D tensor `[batch_size, num_writes, word_size]`.\n",
    "\n",
    "      Returns:\n",
    "            memory: 3-D tensor of shape `[batch_size, num_writes, word_size]`.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('erase_old_memory', \\\n",
    "                           values=[prev_memory, \n",
    "                                   write_weightings, \n",
    "                                   erase_vectors]):\n",
    "\n",
    "            expand_write_weightings = \\\n",
    "            tf.expand_dims(write_weightings, axis=3)\n",
    "\n",
    "            expand_erase_vectors = \\\n",
    "            tf.expand_dims(erase_vectors, axis=2)\n",
    "\n",
    "            # 这里有多个写头，需要使用累成处理多个写头\n",
    "            erase_gates = \\\n",
    "            expand_write_weightings * expand_erase_vectors\n",
    "\n",
    "            retention_gate = \\\n",
    "            tf.reduce_prod(1 - erase_gates, axis=[1])\n",
    "\n",
    "            retention_memory = prev_memory * retention_gate\n",
    "\n",
    "        with tf.name_scope('additive_new_memory', \\\n",
    "                           values=[retention_memory, \n",
    "                                   write_weightings, \n",
    "                                   write_vectors]):\n",
    "\n",
    "            memory = retention_memory + \\\n",
    "            tf.matmul(write_weightings, write_vectors, adjoint_a=True)\n",
    "\n",
    "            return memory\n",
    "        \n",
    "    \n",
    "    def _Calculate_interface_parameters(self, interface_vector):\n",
    "        \"\"\"\n",
    "        Interface parameters. \n",
    "        Before being used to parameterize the memory interactions, \n",
    "        the individual components are then processed with various \n",
    "        functions to ensure that they lie in the correct domain.     \n",
    "        \"\"\"\n",
    "        # read_keys: [batch_size, num_reads, word_size]\n",
    "        read_keys = Linear(\n",
    "            output_size= self._num_reads * self._word_size,\n",
    "            name='read_keys')(interface_vector)\n",
    "        read_keys = tf.reshape(\n",
    "            read_keys, shape=[-1, self._num_reads, self._word_size])\n",
    "\n",
    "        # write_keys: [batch_size, num_writes, word_size]\n",
    "        write_keys = Linear(\n",
    "            output_size= self._num_writes * self._word_size, \n",
    "            name= 'write_keys')(interface_vector)\n",
    "        write_keys = tf.reshape(\n",
    "            write_keys, shape=[-1, self._num_writes, self._word_size])\n",
    "\n",
    "\n",
    "        # read_strengths: [batch_size, num_reads]\n",
    "        read_strengths = Linear(\n",
    "            output_size= self._num_reads,\n",
    "            name= 'read_strengths')(interface_vector)\n",
    "        read_strengths = 1 + tf.nn.softplus(read_strengths)\n",
    "\n",
    "        # write_strengths: [batch_size, num_writes]\n",
    "        write_strengths = Linear(\n",
    "            output_size= self._num_writes,\n",
    "            name='write_strengths')(interface_vector)\n",
    "        write_strengths = 1 + tf.nn.softplus(write_strengths)\n",
    "\n",
    "\n",
    "        # earse_vector: [batch_size, num_writes * word_size]\n",
    "        erase_vectors = Linear(\n",
    "            output_size= self._num_writes * self._word_size,\n",
    "            name='erase_vectors')(interface_vector)\n",
    "        erase_vectors = tf.reshape(\n",
    "            erase_vectors, shape=[-1, self._num_writes, self._word_size])\n",
    "        erase_vectors = tf.nn.sigmoid(erase_vectors)\n",
    "\n",
    "        # write_vectors: [batch_size, num_writes * word_size]\n",
    "        write_vectors = Linear(\n",
    "            output_size= self._num_writes * self._word_size,\n",
    "            name='write_vectors')(interface_vector)\n",
    "        write_vectors = tf.reshape(\n",
    "            write_vectors, shape=[-1, self._num_writes, self._word_size])\n",
    "\n",
    "\n",
    "        # free_gates: [batch_size, num_reads]\n",
    "        free_gates = Linear(\n",
    "            output_size= self._num_reads,\n",
    "            name='free_gates')(interface_vector)\n",
    "        free_gates = tf.nn.sigmoid(free_gates)\n",
    "\n",
    "        # allocation_gates: [batch_size, num_writes]\n",
    "        allocation_gates = Linear(\n",
    "            output_size= self._num_writes,\n",
    "            name='allocation_gates')(interface_vector)\n",
    "        allocation_gates = tf.nn.sigmoid(allocation_gates)\n",
    "\n",
    "        # write_gates: [batch_size, num_writes]\n",
    "        write_gates = Linear(\n",
    "            output_size= self._num_writes,\n",
    "            name='write_gates')(interface_vector)\n",
    "        write_gates = tf.nn.sigmoid(write_gates)\n",
    "\n",
    "        # read_modes: [batch_size, (1 + 2 * num_writes) * num_reads]\n",
    "        num_read_modes = 1 + 2 * self._num_writes\n",
    "        read_modes = Linear(\n",
    "            output_size= self._num_reads * num_read_modes,\n",
    "            name='read_modes')(interface_vector)\n",
    "        read_modes = tf.reshape(\n",
    "            read_modes, shape=[-1, self._num_reads, num_read_modes])\n",
    "        read_modes = BatchApply(tf.nn.softmax)(read_modes)\n",
    "\n",
    "        tape = {\n",
    "            'read_content_keys': read_keys,\n",
    "            'read_content_strengths': read_strengths,\n",
    "            'write_content_keys': write_keys,\n",
    "            'write_content_strengths': write_strengths,\n",
    "            'write_vectors': write_vectors,\n",
    "            'erase_vectors': erase_vectors,\n",
    "            'free_gates': free_gates,\n",
    "            'allocation_gates': allocation_gates,\n",
    "            'write_gates': write_gates,\n",
    "            'read_modes': read_modes,\n",
    "        }\n",
    "        return tape        \n",
    "\n",
    "\n",
    "    def _update_read_weightings(self, \n",
    "                                tape, \n",
    "                                memory, \n",
    "                                write_weightings,\n",
    "                                prev_read_weightings, \n",
    "                                prev_precedence_weightings, \n",
    "                                prev_link):\n",
    "        \"\"\"\n",
    "        Calculates read weights for each read head.\n",
    "\n",
    "        The read weights are a combination of following the link graphs in the\n",
    "        forward or backward directions from the previous read position, and doing\n",
    "        content-based lookup. The interpolation between these different modes is\n",
    "        done by `inputs['read_mode']`.\n",
    "\n",
    "        Args:\n",
    "            inputs: Controls for this access module. \n",
    "            This contains the content-based keys to lookup, \n",
    "            and the weightings for the different read modes.\n",
    "\n",
    "            memory: A tensor of shape `[batch_size, memory_size, word_size]`\n",
    "            containing the current memory contents to do content-based lookup.\n",
    "\n",
    "            prev_read_weights: A tensor of shape `[batch_size, num_reads, memory_size]` \n",
    "            containing the previous read locations.\n",
    "\n",
    "            link: A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n",
    "            containing the temporal write transition graphs.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `[batch_size, num_reads, memory_size]` \n",
    "            containing the read weights for each read head.\n",
    "        \"\"\"    \n",
    "        with tf.name_scope(\n",
    "            'update_read_weightings', \n",
    "            values=[tape, \n",
    "                    memory, \n",
    "                    prev_read_weightings, \n",
    "                    prev_precedence_weightings, \n",
    "                    prev_link]):\n",
    "\n",
    "            read_content_weightings = \\\n",
    "            self._read_content_mod(\n",
    "                memory, \n",
    "                tape['read_content_keys'], \n",
    "                tape['read_content_strengths'])\n",
    "\n",
    "            \n",
    "            link,\\\n",
    "            precedence_weightings,\\\n",
    "            forward_weightings,\\\n",
    "            backward_weightings = \\\n",
    "            self._temporal_linkage(\n",
    "                prev_link, \n",
    "                prev_precedence_weightings, \n",
    "                prev_read_weightings,\n",
    "                write_weightings)\n",
    "            \n",
    "            \n",
    "            backward_mode = tape['read_modes'][:, :, :self._num_writes]\n",
    "            forward_mode = tape['read_modes'][:, :, self._num_writes:2 * self._num_writes]\n",
    "            content_mode = tape['read_modes'][:, :, 2 * self._num_writes]\n",
    "            \n",
    "            backward_ = tf.expand_dims(backward_mode, axis=3) * backward_weightings\n",
    "            backward_ = tf.reduce_sum(backward_, axis=2)\n",
    "            \n",
    "            forward_ = tf.expand_dims(forward_mode, axis=3) * forward_weightings\n",
    "            forward_ = tf.reduce_sum(forward_, axis=2)\n",
    "            \n",
    "            content_ = tf.expand_dims(content_mode, axis=2) * read_content_weightings\n",
    "\n",
    "            read_weightings = backward_ + forward_ + content_\n",
    "\n",
    "            return read_weightings, link, precedence_weightings\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        \"\"\"Returns a tuple of the shape of the state tensors.\"\"\"\n",
    "        memory = tf.TensorShape([self._memory_size, self._word_size])\n",
    "        read_weightings = tf.TensorShape([self._num_reads, self._memory_size])\n",
    "        write_weightings = tf.TensorShape([self._num_writes, self._memory_size])\n",
    "        link = tf.TensorShape([self._num_writes, self._memory_size, self._memory_size])\n",
    "        precedence_weightings = tf.TensorShape([self._num_writes, self._memory_size])\n",
    "        usage = tf.TensorShape([self._memory_size])\n",
    "        return (memory, \n",
    "                read_weightings, \n",
    "                write_weightings,\n",
    "                precedence_weightings,\n",
    "                link, \n",
    "                usage)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        \"\"\"\n",
    "        Returns the output shape.\n",
    "        \"\"\"\n",
    "        return tf.TensorShape([self._num_reads, self._word_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNCore 封装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNCoreLSTM(RNNCore):\n",
    "    \"\"\"\n",
    "    单层LSTM控制器DNCore\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dnc_output_size,\n",
    "        hidden_size= 128,\n",
    "        forget_bias=1.0,\n",
    "        initializers=None,\n",
    "        partitioners=None,\n",
    "        regularizers=None,\n",
    "        use_peepholes=False,\n",
    "        use_layer_norm=False,\n",
    "        hidden_clip_value=None,\n",
    "        cell_clip_value=None,\n",
    "        custom_getter=None,\n",
    "        memory_size= 256,\n",
    "        word_size= 128, \n",
    "        num_read_heads= 3,\n",
    "        num_write_heads= 1,\n",
    "        name='DNCoreLSTM'):\n",
    "        \n",
    "        super().__init__(name=name) # 调用父类初始化\n",
    "        with self._enter_variable_scope():\n",
    "            controller = LSTM(\n",
    "                hidden_size=hidden_size,\n",
    "                forget_bias=forget_bias,\n",
    "                initializers=initializers,\n",
    "                partitioners=partitioners,\n",
    "                regularizers=regularizers,\n",
    "                use_peepholes=use_peepholes,\n",
    "                use_layer_norm=use_layer_norm,\n",
    "                hidden_clip_value=hidden_clip_value,\n",
    "                cell_clip_value=cell_clip_value,\n",
    "                custom_getter=custom_getter)\n",
    "            \n",
    "            self._controller = controller\n",
    "            self._access = MemoryAccess(\n",
    "                memory_size= memory_size, \n",
    "                word_size= word_size, \n",
    "                num_reads= num_read_heads, \n",
    "                num_writes= num_write_heads)  \n",
    "            \n",
    "            \n",
    "        self._dnc_output_size = dnc_output_size\n",
    "        self._num_read_heads = num_read_heads\n",
    "        self._word_size = word_size\n",
    "        \n",
    "        \n",
    "    def _build(self, inputs, prev_tape):\n",
    "        \n",
    "        prev_controller_state,\\\n",
    "        prev_access_state,\\\n",
    "        prev_read_vectors = prev_tape\n",
    "        \n",
    "        batch_flatten = BatchFlatten()\n",
    "        controller_input = tf.concat(\n",
    "            [batch_flatten(inputs), batch_flatten(prev_read_vectors)], axis= 1)\n",
    "        \n",
    "        # 控制器处理数据\n",
    "        controller_output, controller_state = \\\n",
    "        self._controller(controller_input, prev_controller_state)\n",
    "        \n",
    "        # 外存储器交互\n",
    "        read_vectors, access_state = \\\n",
    "        self._access(controller_output, prev_access_state)\n",
    "        \n",
    "        # DNC 输出\n",
    "        dnc_output = tf.concat(\n",
    "            [controller_output, batch_flatten(read_vectors)], axis= 1)\n",
    "        dnc_output = Linear(\n",
    "            self._dnc_output_size, name='dnc_output')(dnc_output)\n",
    "        \n",
    "        return dnc_output, (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    def initial_state(self, batch_size, dtype=tf.float32):\n",
    "        controller_state= self._controller.initial_state(batch_size, dtype)\n",
    "        access_state= self._access.initial_state(batch_size, dtype)\n",
    "        read_vectors= tf.zeros([batch_size, self._num_read_heads, self._word_size], dtype=dtype)\n",
    "        return (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        controller_state= self._controller.state_size\n",
    "        access_state= self._access.state_size\n",
    "        read_vectors= tf.TensorShape([self._num_read_heads, self._word_size])\n",
    "        return (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return tf.TensorShape([self._dnc_output_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DeepRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNCoreDeepLSTM(RNNCore):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dnc_output_size,\n",
    "        hidden_size= 128,\n",
    "        forget_bias=1.0,\n",
    "        initializers=None,\n",
    "        partitioners=None,\n",
    "        regularizers=None,\n",
    "        use_peepholes=False,\n",
    "        use_layer_norm=False,\n",
    "        hidden_clip_value=None,\n",
    "        cell_clip_value=None,\n",
    "        custom_getter=None,\n",
    "        memory_size= 256,\n",
    "        word_size= 128, \n",
    "        num_read_heads= 3,\n",
    "        num_write_heads= 1,\n",
    "        name='DNCoreDeepLSTM'):\n",
    "        \n",
    "        super().__init__(name=name) # 调用父类初始化\n",
    "        with self._enter_variable_scope():\n",
    "            \n",
    "            layer_1 = LSTM(\n",
    "                hidden_size=hidden_size,\n",
    "                forget_bias=forget_bias,\n",
    "                initializers=initializers,\n",
    "                partitioners=partitioners,\n",
    "                regularizers=regularizers,\n",
    "                use_peepholes=use_peepholes,\n",
    "                use_layer_norm=use_layer_norm,\n",
    "                hidden_clip_value=hidden_clip_value,\n",
    "                cell_clip_value=cell_clip_value,\n",
    "                custom_getter=custom_getter)\n",
    "            \n",
    "            layer_2 = LSTM(\n",
    "                hidden_size=hidden_size,\n",
    "                forget_bias=forget_bias,\n",
    "                initializers=initializers,\n",
    "                partitioners=partitioners,\n",
    "                regularizers=regularizers,\n",
    "                use_peepholes=use_peepholes,\n",
    "                use_layer_norm=use_layer_norm,\n",
    "                hidden_clip_value=hidden_clip_value,\n",
    "                cell_clip_value=cell_clip_value,\n",
    "                custom_getter=custom_getter)\n",
    "            \n",
    "            layer_3 = LSTM(\n",
    "                hidden_size=hidden_size,\n",
    "                forget_bias=forget_bias,\n",
    "                initializers=initializers,\n",
    "                partitioners=partitioners,\n",
    "                regularizers=regularizers,\n",
    "                use_peepholes=use_peepholes,\n",
    "                use_layer_norm=use_layer_norm,\n",
    "                hidden_clip_value=hidden_clip_value,\n",
    "                cell_clip_value=cell_clip_value,\n",
    "                custom_getter=custom_getter)\n",
    "\n",
    "            self._controller = DeepRNN([layer_1, layer_2, layer_3]) \n",
    "            self._access = MemoryAccess(\n",
    "                memory_size= memory_size, \n",
    "                word_size= word_size, \n",
    "                num_reads= num_read_heads, \n",
    "                num_writes= num_write_heads)\n",
    "            \n",
    "        self._dnc_output_size = dnc_output_size\n",
    "        self._num_read_heads = num_read_heads\n",
    "        self._word_size = word_size\n",
    "        \n",
    "        \n",
    "    def _build(self, inputs, prev_tape):\n",
    "        \n",
    "        prev_controller_state,\\\n",
    "        prev_access_state,\\\n",
    "        prev_read_vectors = prev_tape\n",
    "        \n",
    "        batch_flatten = BatchFlatten()\n",
    "        controller_input = tf.concat(\n",
    "            [batch_flatten(inputs), batch_flatten(prev_read_vectors)], axis= 1)\n",
    "        \n",
    "        # 控制器处理数据\n",
    "        controller_output, controller_state = \\\n",
    "        self._controller(controller_input, prev_controller_state)\n",
    "        \n",
    "        # 外存储器交互\n",
    "        read_vectors, access_state = \\\n",
    "        self._access(controller_output, prev_access_state)\n",
    "        \n",
    "        # DNC 输出\n",
    "        dnc_output = tf.concat(\n",
    "            [controller_output, batch_flatten(read_vectors)], axis= 1)\n",
    "        dnc_output = Linear(\n",
    "            self._dnc_output_size, name='dnc_output')(dnc_output)\n",
    "        \n",
    "        return dnc_output, (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    def initial_state(self, batch_size, dtype=tf.float32):\n",
    "        controller_state= self._controller.initial_state(batch_size, dtype)\n",
    "        access_state= self._access.initial_state(batch_size, dtype)\n",
    "        read_vectors= tf.zeros([batch_size, self._num_read_heads, self._word_size], dtype=dtype)\n",
    "        return (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        controller_state= self._controller.state_size\n",
    "        access_state= self._access.state_size\n",
    "        read_vectors= tf.TensorShape([self._num_read_heads, self._word_size])\n",
    "        return (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return tf.TensorShape([self._dnc_output_size])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
