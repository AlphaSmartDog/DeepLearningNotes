{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = np.random.normal(scale=2, size=(800, 69))\n",
    "train_y = np.random.randint(0, high=10, size=(800,1), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch (1000/100000), loss:2.8511\n",
      "Epoch (2000/100000), loss:2.7817\n",
      "Epoch (3000/100000), loss:2.7173\n",
      "Epoch (4000/100000), loss:2.6575\n",
      "Epoch (5000/100000), loss:2.6021\n",
      "Epoch (6000/100000), loss:2.5507\n",
      "Epoch (7000/100000), loss:2.5032\n",
      "Epoch (8000/100000), loss:2.4591\n",
      "Epoch (9000/100000), loss:2.4184\n",
      "Epoch (10000/100000), loss:2.3806\n",
      "Epoch (11000/100000), loss:2.3457\n",
      "Epoch (12000/100000), loss:2.3133\n",
      "Epoch (13000/100000), loss:2.2834\n",
      "Epoch (14000/100000), loss:2.2556\n",
      "Epoch (15000/100000), loss:2.2299\n",
      "Epoch (16000/100000), loss:2.2061\n",
      "Epoch (17000/100000), loss:2.1840\n",
      "Epoch (18000/100000), loss:2.1636\n",
      "Epoch (19000/100000), loss:2.1446\n",
      "Epoch (20000/100000), loss:2.1270\n",
      "Epoch (21000/100000), loss:2.1106\n",
      "Epoch (22000/100000), loss:2.0954\n",
      "Epoch (23000/100000), loss:2.0813\n",
      "Epoch (24000/100000), loss:2.0681\n",
      "Epoch (25000/100000), loss:2.0559\n",
      "Epoch (26000/100000), loss:2.0446\n",
      "Epoch (27000/100000), loss:2.0340\n",
      "Epoch (28000/100000), loss:2.0241\n",
      "Epoch (29000/100000), loss:2.0149\n",
      "Epoch (30000/100000), loss:2.0063\n",
      "Epoch (31000/100000), loss:1.9983\n",
      "Epoch (32000/100000), loss:1.9908\n",
      "Epoch (33000/100000), loss:1.9838\n",
      "Epoch (34000/100000), loss:1.9772\n",
      "Epoch (35000/100000), loss:1.9711\n",
      "Epoch (36000/100000), loss:1.9653\n",
      "Epoch (37000/100000), loss:1.9599\n",
      "Epoch (38000/100000), loss:1.9548\n",
      "Epoch (39000/100000), loss:1.9501\n",
      "Epoch (40000/100000), loss:1.9456\n",
      "Epoch (41000/100000), loss:1.9414\n",
      "Epoch (42000/100000), loss:1.9374\n",
      "Epoch (43000/100000), loss:1.9336\n",
      "Epoch (44000/100000), loss:1.9301\n",
      "Epoch (45000/100000), loss:1.9268\n",
      "Epoch (46000/100000), loss:1.9237\n",
      "Epoch (47000/100000), loss:1.9207\n",
      "Epoch (48000/100000), loss:1.9179\n",
      "Epoch (49000/100000), loss:1.9152\n",
      "Epoch (50000/100000), loss:1.9127\n",
      "Epoch (51000/100000), loss:1.9103\n",
      "Epoch (52000/100000), loss:1.9080\n",
      "Epoch (53000/100000), loss:1.9059\n",
      "Epoch (54000/100000), loss:1.9039\n",
      "Epoch (55000/100000), loss:1.9019\n",
      "Epoch (56000/100000), loss:1.9001\n",
      "Epoch (57000/100000), loss:1.8983\n",
      "Epoch (58000/100000), loss:1.8967\n",
      "Epoch (59000/100000), loss:1.8951\n",
      "Epoch (60000/100000), loss:1.8936\n",
      "Epoch (61000/100000), loss:1.8921\n",
      "Epoch (62000/100000), loss:1.8907\n",
      "Epoch (63000/100000), loss:1.8894\n",
      "Epoch (64000/100000), loss:1.8882\n",
      "Epoch (65000/100000), loss:1.8870\n",
      "Epoch (66000/100000), loss:1.8858\n",
      "Epoch (67000/100000), loss:1.8847\n",
      "Epoch (68000/100000), loss:1.8837\n",
      "Epoch (69000/100000), loss:1.8826\n",
      "Epoch (70000/100000), loss:1.8817\n",
      "Epoch (71000/100000), loss:1.8808\n",
      "Epoch (72000/100000), loss:1.8799\n",
      "Epoch (73000/100000), loss:1.8790\n",
      "Epoch (74000/100000), loss:1.8782\n",
      "Epoch (75000/100000), loss:1.8774\n",
      "Epoch (76000/100000), loss:1.8767\n",
      "Epoch (77000/100000), loss:1.8759\n",
      "Epoch (78000/100000), loss:1.8752\n",
      "Epoch (79000/100000), loss:1.8746\n",
      "Epoch (80000/100000), loss:1.8739\n",
      "Epoch (81000/100000), loss:1.8733\n",
      "Epoch (82000/100000), loss:1.8727\n",
      "Epoch (83000/100000), loss:1.8721\n",
      "Epoch (84000/100000), loss:1.8716\n",
      "Epoch (85000/100000), loss:1.8711\n",
      "Epoch (86000/100000), loss:1.8705\n",
      "Epoch (87000/100000), loss:1.8700\n",
      "Epoch (88000/100000), loss:1.8696\n",
      "Epoch (89000/100000), loss:1.8691\n",
      "Epoch (90000/100000), loss:1.8687\n",
      "Epoch (91000/100000), loss:1.8682\n",
      "Epoch (92000/100000), loss:1.8678\n",
      "Epoch (93000/100000), loss:1.8674\n",
      "Epoch (94000/100000), loss:1.8670\n",
      "Epoch (95000/100000), loss:1.8666\n",
      "Epoch (96000/100000), loss:1.8663\n",
      "Epoch (97000/100000), loss:1.8659\n",
      "Epoch (98000/100000), loss:1.8656\n",
      "Epoch (99000/100000), loss:1.8653\n",
      "Epoch (100000/100000), loss:1.8649\n"
     ]
    }
   ],
   "source": [
    "class LogistRegression(nn.Module): #  所有网络的基类。\n",
    "    \n",
    "    def __init__(self, inputs, targets, targets_size=10, learning_rate=1e-4):\n",
    "        super().__init__() # 在子类中调用父类的初始化方法\n",
    "        self._train_X = inputs\n",
    "        self._train_y = targets\n",
    "        self._train_X_size = inputs.shape[1]\n",
    "        self._train_y_size = targets_size\n",
    "        self._learning_rate = learning_rate    \n",
    "        \n",
    "        self._linear = nn.Linear(self._train_X_size, self._train_y_size)\n",
    "        self._loss_function = nn.CrossEntropyLoss()\n",
    "        self._optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "        \n",
    "    def fit(self, training_epochs= 1e3, display= 1e2):\n",
    "        display = np.int(display)\n",
    "        for epoch in np.arange(np.int(training_epochs)):\n",
    "            inputs = Variable(torch.FloatTensor(self._train_X),requires_grad=True)\n",
    "            targets = Variable(torch.LongTensor(self._train_y.flatten()))\n",
    "            self._optimizer.zero_grad() #清空所有被优化过的Variable的梯度.\n",
    "            outputs = self._linear(inputs) # 使用神经网络架构前向推断\n",
    "            self._loss = self._loss_function(outputs, targets) # 计算批次损失函数\n",
    "            self._loss.backward() # 误差反向传播\n",
    "            self._optimizer.step()\n",
    "            \n",
    "            if (epoch+1) % display == 0:\n",
    "                print ('Epoch (%d/%d), loss:%.4f' %(epoch+1, training_epochs, self._loss.data[0]))    \n",
    "    \n",
    "    def pred(self, X):\n",
    "        outputs = self._linear(Variable(torch.FloatTensor(X)))\n",
    "        _, output_labels  = torch.max(outputs, 1)\n",
    "        return output_labels\n",
    "            \n",
    "a = LogistRegression(train_X, train_y, 10)\n",
    "a.fit(1e5, 1e3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 3\n",
       " 5\n",
       " 4\n",
       " 9\n",
       " 4\n",
       " 4\n",
       " 0\n",
       " 8\n",
       " 3\n",
       " 8\n",
       " 0\n",
       " 3\n",
       " 2\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 7\n",
       " 0\n",
       " 8\n",
       " 8\n",
       " 8\n",
       " 5\n",
       " 5\n",
       " 5\n",
       " 4\n",
       " 1\n",
       " 8\n",
       " 8\n",
       " 3\n",
       " 6\n",
       " 6\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 7\n",
       " 7\n",
       " 3\n",
       " 9\n",
       " 1\n",
       " 1\n",
       " 5\n",
       " 6\n",
       " 3\n",
       " 1\n",
       " 6\n",
       " 1\n",
       " 3\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 3\n",
       " 4\n",
       " 1\n",
       " 1\n",
       " 6\n",
       " 4\n",
       " 8\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 3\n",
       " 6\n",
       " 0\n",
       " 7\n",
       " 8\n",
       " 3\n",
       " 6\n",
       " 7\n",
       " 4\n",
       " 6\n",
       " 8\n",
       " 0\n",
       " 5\n",
       " 3\n",
       " 3\n",
       " 5\n",
       " 4\n",
       " 1\n",
       " 7\n",
       " 2\n",
       " 1\n",
       " 7\n",
       " 4\n",
       " 0\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 7\n",
       " 7\n",
       " 1\n",
       " 7\n",
       " 0\n",
       " 5\n",
       " 3\n",
       " 3\n",
       " 7\n",
       " 9\n",
       " 0\n",
       " 9\n",
       " 6\n",
       " 3\n",
       " 3\n",
       " 6\n",
       " 1\n",
       " 2\n",
       " 8\n",
       " 4\n",
       " 1\n",
       " 5\n",
       " 4\n",
       " 2\n",
       " 8\n",
       " 8\n",
       " 4\n",
       " 7\n",
       " 3\n",
       " 9\n",
       " 5\n",
       " 8\n",
       " 1\n",
       " 4\n",
       " 0\n",
       " 5\n",
       " 7\n",
       " 8\n",
       " 2\n",
       " 6\n",
       " 2\n",
       " 3\n",
       " 8\n",
       " 3\n",
       " 7\n",
       " 4\n",
       " 5\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 8\n",
       " 5\n",
       " 4\n",
       " 4\n",
       " 9\n",
       " 0\n",
       " 4\n",
       " 4\n",
       " 7\n",
       " 1\n",
       " 3\n",
       " 5\n",
       " 9\n",
       " 6\n",
       " 7\n",
       " 4\n",
       " 9\n",
       " 5\n",
       " 8\n",
       " 2\n",
       " 4\n",
       " 1\n",
       " 4\n",
       " 4\n",
       " 3\n",
       " 8\n",
       " 4\n",
       " 7\n",
       " 5\n",
       " 3\n",
       " 1\n",
       " 9\n",
       " 6\n",
       " 1\n",
       " 5\n",
       " 1\n",
       " 6\n",
       " 3\n",
       " 8\n",
       " 0\n",
       " 8\n",
       " 6\n",
       " 1\n",
       " 9\n",
       " 9\n",
       " 8\n",
       " 9\n",
       " 0\n",
       " 4\n",
       " 5\n",
       " 1\n",
       " 9\n",
       " 3\n",
       " 2\n",
       " 0\n",
       " 4\n",
       " 0\n",
       " 1\n",
       " 8\n",
       " 5\n",
       " 4\n",
       " 2\n",
       " 3\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 4\n",
       " 9\n",
       " 0\n",
       " 2\n",
       " 3\n",
       " 3\n",
       " 0\n",
       " 5\n",
       " 6\n",
       " 1\n",
       " 7\n",
       " 9\n",
       " 4\n",
       " 0\n",
       " 5\n",
       " 0\n",
       " 9\n",
       " 3\n",
       " 0\n",
       " 8\n",
       " 2\n",
       " 7\n",
       " 9\n",
       " 3\n",
       " 0\n",
       " 8\n",
       " 0\n",
       " 0\n",
       " 6\n",
       " 9\n",
       " 0\n",
       " 8\n",
       " 6\n",
       " 4\n",
       " 1\n",
       " 7\n",
       " 4\n",
       " 8\n",
       " 8\n",
       " 6\n",
       " 5\n",
       " 8\n",
       " 8\n",
       " 1\n",
       " 0\n",
       " 9\n",
       " 5\n",
       " 4\n",
       " 7\n",
       " 2\n",
       " 4\n",
       " 6\n",
       " 3\n",
       " 9\n",
       " 2\n",
       " 3\n",
       " 3\n",
       " 8\n",
       " 3\n",
       " 7\n",
       " 9\n",
       " 5\n",
       " 3\n",
       " 9\n",
       " 5\n",
       " 1\n",
       " 1\n",
       " 3\n",
       " 0\n",
       " 9\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 3\n",
       " 7\n",
       " 8\n",
       " 4\n",
       " 5\n",
       " 8\n",
       " 1\n",
       " 2\n",
       " 2\n",
       " 3\n",
       " 1\n",
       " 4\n",
       " 1\n",
       " 5\n",
       " 3\n",
       " 4\n",
       " 9\n",
       " 3\n",
       " 7\n",
       " 3\n",
       " 5\n",
       " 9\n",
       " 5\n",
       " 7\n",
       " 4\n",
       " 5\n",
       " 3\n",
       " 5\n",
       " 3\n",
       " 8\n",
       " 8\n",
       " 1\n",
       " 4\n",
       " 1\n",
       " 5\n",
       " 3\n",
       " 3\n",
       " 7\n",
       " 9\n",
       " 4\n",
       " 8\n",
       " 7\n",
       " 9\n",
       " 6\n",
       " 1\n",
       " 7\n",
       " 7\n",
       " 3\n",
       " 1\n",
       " 5\n",
       " 2\n",
       " 6\n",
       " 6\n",
       " 6\n",
       " 4\n",
       " 0\n",
       " 8\n",
       " 3\n",
       " 9\n",
       " 2\n",
       " 7\n",
       " 8\n",
       " 0\n",
       " 0\n",
       " 4\n",
       " 5\n",
       " 8\n",
       " 2\n",
       " 5\n",
       " 7\n",
       " 7\n",
       " 7\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 5\n",
       " 7\n",
       " 3\n",
       " 3\n",
       " 3\n",
       " 4\n",
       " 8\n",
       " 2\n",
       " 8\n",
       " 9\n",
       " 8\n",
       " 6\n",
       " 9\n",
       " 1\n",
       " 5\n",
       " 6\n",
       " 8\n",
       " 0\n",
       " 8\n",
       " 1\n",
       " 4\n",
       " 8\n",
       " 4\n",
       " 8\n",
       " 7\n",
       " 2\n",
       " 7\n",
       " 7\n",
       " 7\n",
       " 7\n",
       " 3\n",
       " 5\n",
       " 9\n",
       " 7\n",
       " 1\n",
       " 9\n",
       " 7\n",
       " 1\n",
       " 9\n",
       " 5\n",
       " 0\n",
       " 4\n",
       " 7\n",
       " 9\n",
       " 5\n",
       " 6\n",
       " 0\n",
       " 7\n",
       " 7\n",
       " 3\n",
       " 1\n",
       " 5\n",
       " 5\n",
       " 3\n",
       " 6\n",
       " 5\n",
       " 4\n",
       " 7\n",
       " 5\n",
       " 8\n",
       " 9\n",
       " 4\n",
       " 9\n",
       " 5\n",
       " 1\n",
       " 0\n",
       " 8\n",
       " 2\n",
       " 3\n",
       " 3\n",
       " 0\n",
       " 9\n",
       " 1\n",
       " 4\n",
       " 0\n",
       " 6\n",
       " 8\n",
       " 4\n",
       " 2\n",
       " 4\n",
       " 3\n",
       " 4\n",
       " 6\n",
       " 4\n",
       " 3\n",
       " 9\n",
       " 3\n",
       " 8\n",
       " 2\n",
       " 3\n",
       " 3\n",
       " 9\n",
       " 0\n",
       " 1\n",
       " 4\n",
       " 8\n",
       " 7\n",
       " 0\n",
       " 8\n",
       " 3\n",
       " 7\n",
       " 4\n",
       " 9\n",
       " 6\n",
       " 2\n",
       " 2\n",
       " 3\n",
       " 8\n",
       " 4\n",
       " 1\n",
       " 5\n",
       " 2\n",
       " 8\n",
       " 5\n",
       " 2\n",
       " 0\n",
       " 6\n",
       " 4\n",
       " 0\n",
       " 8\n",
       " 6\n",
       " 2\n",
       " 9\n",
       " 3\n",
       " 3\n",
       " 6\n",
       " 5\n",
       " 1\n",
       " 7\n",
       " 7\n",
       " 3\n",
       " 2\n",
       " 5\n",
       " 1\n",
       " 0\n",
       " 5\n",
       " 3\n",
       " 6\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 5\n",
       " 3\n",
       " 9\n",
       " 3\n",
       " 1\n",
       " 8\n",
       " 0\n",
       " 5\n",
       " 1\n",
       " 3\n",
       " 8\n",
       " 3\n",
       " 2\n",
       " 9\n",
       " 9\n",
       " 8\n",
       " 3\n",
       " 3\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 9\n",
       " 1\n",
       " 4\n",
       " 4\n",
       " 8\n",
       " 8\n",
       " 5\n",
       " 2\n",
       " 1\n",
       " 4\n",
       " 8\n",
       " 2\n",
       " 8\n",
       " 0\n",
       " 8\n",
       " 9\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 1\n",
       " 9\n",
       " 3\n",
       " 1\n",
       " 7\n",
       " 7\n",
       " 9\n",
       " 7\n",
       " 8\n",
       " 8\n",
       " 5\n",
       " 4\n",
       " 3\n",
       " 0\n",
       " 3\n",
       " 3\n",
       " 0\n",
       " 6\n",
       " 1\n",
       " 5\n",
       " 3\n",
       " 7\n",
       " 0\n",
       " 3\n",
       " 4\n",
       " 8\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 3\n",
       " 2\n",
       " 0\n",
       " 5\n",
       " 1\n",
       " 8\n",
       " 9\n",
       " 8\n",
       " 7\n",
       " 2\n",
       " 6\n",
       " 9\n",
       " 3\n",
       " 6\n",
       " 2\n",
       " 3\n",
       " 1\n",
       " 3\n",
       " 1\n",
       " 7\n",
       " 0\n",
       " 2\n",
       " 5\n",
       " 4\n",
       " 6\n",
       " 3\n",
       " 7\n",
       " 0\n",
       " 3\n",
       " 6\n",
       " 5\n",
       " 7\n",
       " 1\n",
       " 1\n",
       " 8\n",
       " 4\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 0\n",
       " 9\n",
       " 6\n",
       " 6\n",
       " 5\n",
       " 1\n",
       " 8\n",
       " 4\n",
       " 8\n",
       " 9\n",
       " 4\n",
       " 4\n",
       " 3\n",
       " 6\n",
       " 3\n",
       " 6\n",
       " 6\n",
       " 7\n",
       " 0\n",
       " 3\n",
       " 6\n",
       " 2\n",
       " 4\n",
       " 4\n",
       " 0\n",
       " 3\n",
       " 9\n",
       " 2\n",
       " 9\n",
       " 4\n",
       " 4\n",
       " 0\n",
       " 0\n",
       " 5\n",
       " 6\n",
       " 9\n",
       " 8\n",
       " 3\n",
       " 7\n",
       " 1\n",
       " 6\n",
       " 8\n",
       " 5\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 1\n",
       " 6\n",
       " 3\n",
       " 6\n",
       " 4\n",
       " 0\n",
       " 9\n",
       " 9\n",
       " 6\n",
       " 9\n",
       " 7\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 0\n",
       " 8\n",
       " 5\n",
       " 8\n",
       " 8\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 8\n",
       " 0\n",
       " 8\n",
       " 0\n",
       " 6\n",
       " 3\n",
       " 4\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 0\n",
       " 7\n",
       " 8\n",
       " 1\n",
       " 7\n",
       " 4\n",
       " 7\n",
       " 1\n",
       " 2\n",
       " 8\n",
       " 8\n",
       " 7\n",
       " 1\n",
       " 6\n",
       " 3\n",
       " 7\n",
       " 8\n",
       " 5\n",
       " 0\n",
       " 5\n",
       " 4\n",
       " 5\n",
       " 4\n",
       " 5\n",
       " 9\n",
       " 2\n",
       " 2\n",
       " 5\n",
       " 7\n",
       " 2\n",
       " 3\n",
       " 0\n",
       " 0\n",
       " 8\n",
       " 5\n",
       " 8\n",
       " 9\n",
       " 7\n",
       " 5\n",
       " 6\n",
       " 6\n",
       " 8\n",
       " 8\n",
       " 6\n",
       " 3\n",
       " 8\n",
       " 9\n",
       " 8\n",
       " 1\n",
       " 0\n",
       " 4\n",
       " 8\n",
       " 8\n",
       " 9\n",
       " 4\n",
       " 3\n",
       " 6\n",
       " 0\n",
       " 4\n",
       " 1\n",
       " 2\n",
       " 8\n",
       " 0\n",
       " 1\n",
       " 7\n",
       " 6\n",
       " 4\n",
       " 3\n",
       " 5\n",
       " 2\n",
       " 7\n",
       " 8\n",
       " 1\n",
       " 5\n",
       " 8\n",
       " 7\n",
       " 3\n",
       " 0\n",
       " 5\n",
       " 2\n",
       " 7\n",
       " 8\n",
       " 8\n",
       " 4\n",
       " 8\n",
       " 6\n",
       " 5\n",
       " 4\n",
       " 3\n",
       " 3\n",
       " 6\n",
       " 7\n",
       " 6\n",
       " 4\n",
       " 1\n",
       " 4\n",
       " 9\n",
       " 2\n",
       " 8\n",
       " 5\n",
       " 0\n",
       " 5\n",
       " 1\n",
       " 2\n",
       " 6\n",
       " 7\n",
       " 4\n",
       " 5\n",
       " 0\n",
       " 1\n",
       " 8\n",
       " 0\n",
       " 3\n",
       " 8\n",
       " 2\n",
       " 4\n",
       " 5\n",
       " 8\n",
       " 9\n",
       " 7\n",
       " 8\n",
       "[torch.LongTensor of size 800]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.pred(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.max?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
