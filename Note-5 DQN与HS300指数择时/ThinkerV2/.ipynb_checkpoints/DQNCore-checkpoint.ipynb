{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DQNCore(object):\n",
    "    def __init__(self, \n",
    "                 observation, \n",
    "                 num_actions, \n",
    "                 memory_size=1024, \n",
    "                 batch_size=32, \n",
    "                 gamma=.9, \n",
    "                 learning_rate=1e-3,\n",
    "                 optimizer_epsilon = 1e-8,\n",
    "                 l2_coefficient=1e-3,\n",
    "                 max_gard_norm=50,\n",
    "                 name='DNCore'):\n",
    "        self.num_actions = num_actions\n",
    "        self.memory_size = memory_size\n",
    "        self.gamma = gamma # discount factor for excepted returns \n",
    "        self.batch_size = 32\n",
    "        \n",
    "        # placeholder for samples replay experience\n",
    "        shape = [None] + list(observation.shape [1:])\n",
    "        self.inputs = tf.placeholder(tf.float32, shape, 'inputs')\n",
    "        self.targets = tf.placeholder(tf.float32, [None], 'targets') # y_j\n",
    "        self.actions = tf.placeholder(tf.int32, [None], 'actions')\n",
    "        self.rewards = tf.placeholder(tf.float32, [None], 'rewards')\n",
    "        self.Q = self._build_QNetwork('Qeval', True) # state Q\n",
    "        self.next_Q = self._build_QNetwork('next_eval',False) # next state Q\n",
    "        \n",
    "        # actions selection corresponding one hot matrix column\n",
    "        one_hot = tf.one_hot(self.actions, self.num_actions, 1., 0.)\n",
    "        Qmax = tf.reduce_sum(self.Q * one_hot, axis=1)\n",
    "        cost = tf.reduce_mean(tf.squared_difference(Qmax, self.targets))\n",
    "        # L2 正则化\n",
    "        self._trainable_variables = tf.trainable_variables()\n",
    "        _l2_regularizer = tf.add_n([tf.nn.l2_loss(v) for v in self._trainable_variables])        \n",
    "        self._l2_regularizer = _l2_regularizer * l2_coefficient / len(self._trainable_variables)\n",
    "        self._loss = self._l2_regularizer + cost\n",
    "        \n",
    "        # Set up optimizer with global norm clipping.\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self._loss, trainable_variables), max_gard_norm)\n",
    "        global_step = tf.get_variable(\n",
    "            name=\"global_step\",\n",
    "            shape=[],\n",
    "            dtype=tf.int64,\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])       \n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=learning_rate, epsilon=optimizer_epsilon)\n",
    "        self._train_op = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables), global_step=global_step)  \n",
    "\n",
    "        # update target network\n",
    "        next_params = tf.get_collection(\n",
    "            tf.GraphKeys.GLOBAL_VARIABLES, \n",
    "            scope='next_eval')\n",
    "        Q_params = tf.get_collection(\n",
    "            tf.GraphKeys.GLOBAL_VARIABLES, \n",
    "            scope='Qeval')\n",
    "        self._update_target = [tf.assign(n,q) for n,q in zip(next_params, Q_params)]\n",
    "        \n",
    "        # session\n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())   \n",
    "        self.sess.graph.finalize()\n",
    "        \n",
    "    def init(self):\n",
    "        self.step_counter = 0       \n",
    "        self.cache = deque(maxlen=self.memory_size) # replay experience\n",
    "\n",
    "    def _build_QNetwork(self, name, trainable):\n",
    "        with tf.variable_scope(name):\n",
    "            # input layer\n",
    "            network = tf.layers.conv2d(self.inputs, 32, [8,8], [4,4], 'same', \n",
    "                                       activation=tf.nn.relu, trainable=trainable, name='input_layer')\n",
    "            # hidden layer\n",
    "            network = tf.layers.conv2d(network, 64, [4,4], [2,2], 'same', \n",
    "                                       activation=tf.nn.relu, trainable=trainable, name='hidden_layer')\n",
    "            # hidden layer 2nd\n",
    "            network = tf.layers.conv2d(network, 64, [3,3], [1,1], 'same', \n",
    "                                       activation=tf.nn.relu, trainable=trainable, name='hidden_layer_2nd')\n",
    "            # final layer\n",
    "            network = tf.contrib.layers.flatten(network)\n",
    "            network = tf.layers.dense(network, 512, tf.nn.relu, \n",
    "                                      trainable=trainable, name='final_layer')\n",
    "            # output layer\n",
    "            network = tf.layers.dense(network, self.num_actions, None, \n",
    "                                      trainable=trainable, name='output_layer')\n",
    "            return network\n",
    "\n",
    "    def update_nextQ_network(self): \n",
    "        # zip 长度不等时，取长度的最小的\n",
    "        self.sess.run(self._update_target)\n",
    "\n",
    "    def update_cache(self, state, action, reward, next_state, done):\n",
    "        # update replay experience pool\n",
    "        self.cache.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def _get_minibatch(self):\n",
    "        # get samples from replay experience pool\n",
    "        minibatch = random.sample(self.cache, self.batch_size) \n",
    "        state = np.vstack([i[0] for i in minibatch])\n",
    "        action = np.squeeze(np.vstack([i[1] for i in minibatch]))\n",
    "        reward = np.squeeze(np.vstack([i[2] for i in minibatch]))\n",
    "        next_state = np.vstack([i[3] for i in minibatch])\n",
    "        done = [i[4] for i in minibatch]\n",
    "        return state, action, reward, next_state, done\n",
    "\n",
    "    def step_learning(self):\n",
    "        # samples from repaly experience pool\n",
    "        state, action, reward, next_state, done = self._get_minibatch()\n",
    "        next_Q = self.sess.run(self.next_Q, feed_dict={self.inputs:next_state})\n",
    "        # done mask True 1 False 0\n",
    "        mask = np.array(done).astype('float')\n",
    "        target = mask * reward + (1 - mask) * \\\n",
    "        (reward + self.gamma * np.max(next_Q, axis=1))\n",
    "        \n",
    "        # op gradient descent step \n",
    "        self.sess.run(self._train_op, \n",
    "                      feed_dict={self.inputs:state, \n",
    "                                 self.actions:action, \n",
    "                                 self.targets:target})    \n",
    "        \n",
    "    def greedy_policy(self, observation):\n",
    "        # 注：只在优化逼近函数参数过程使用 varepsilon greedy policy\n",
    "        action_value = self.sess.run(\n",
    "            self.Q, feed_dict={self.inputs:observation})\n",
    "        return np.argmax(action_value, axis=1)[0]\n",
    "    \n",
    "    def varepsilon_greedy_policy(self, observation, varepsilon=0.9):\n",
    "        if np.random.uniform() < varepsilon:\n",
    "            action = self.greedy_policy(observation)\n",
    "        else:\n",
    "            action = np.random.randint(self.num_actions)\n",
    "        return action"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
