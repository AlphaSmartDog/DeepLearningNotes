{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = np.random.normal(scale=2, size=(800, 69))\n",
    "train_y = np.random.randint(0, high=10, size=(800,1), dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayers(nn.Module):\n",
    "    \n",
    "    def __init__(self, inputs, targets, targets_size=10, learning_rate=1e-4):\n",
    "        super().__init__() # 在子类中调用父类的初始化方法\n",
    "        self._train_X = inputs\n",
    "        self._train_y = targets\n",
    "        self._train_X_size = inputs.shape[1]\n",
    "        self._train_y_size = targets_size\n",
    "        self._learning_rate = learning_rate    \n",
    "        \n",
    "        self._fc1 = nn.Linear(self._train_X_size, self._train_X_size)\n",
    "        self._ac1 = nn.ReLU() # activation function\n",
    "        self._fc2 = nn.Linear(self._train_X_size, self._train_y_size)\n",
    "        \n",
    "        self._loss_function = nn.CrossEntropyLoss()\n",
    "        self._optimizer = torch.optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self._fc1(X)\n",
    "        out = self._ac1(out)\n",
    "        out = self._fc2(out)\n",
    "        return out\n",
    "    \n",
    "    def fit(self, training_epochs= 1e3, display= 1e2):\n",
    "        display = np.int(display)\n",
    "        for epoch in np.arange(np.int(training_epochs)):\n",
    "            inputs = Variable(torch.FloatTensor(self._train_X),requires_grad=True)\n",
    "            targets = Variable(torch.LongTensor(self._train_y.flatten()))\n",
    "            self._optimizer.zero_grad() #清空所有被优化过的Variable的梯度.\n",
    "            outputs = self.forward(inputs) # 使用神经网络架构前向推断\n",
    "            self._loss = self._loss_function(outputs, targets) # 计算批次损失函数\n",
    "            self._loss.backward() # 误差反向传播\n",
    "            self._optimizer.step()\n",
    "            \n",
    "            if (epoch+1) % display == 0:\n",
    "                print ('Epoch (%d/%d), loss:%.4f' %(epoch+1, training_epochs, self._loss.data[0]))    \n",
    "    \n",
    "    def pred(self,X):\n",
    "        inputs = (Variable(torch.FloatTensor(X)))\n",
    "        outputs = self.forward(inputs)\n",
    "        _, output_labels  = torch.max(outputs, 1)\n",
    "        return output_labels\n",
    "    \n",
    "    \n",
    "a = FullyConnectedLayers(train_X, train_y, 10)\n",
    "a.fit(1e3, 1e2)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.pred(train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
