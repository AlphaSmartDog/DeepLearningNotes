{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import functools\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sonnet.python.modules import basic\n",
    "from sonnet.python.modules import rnn_core\n",
    "from sonnet.python.ops import nest\n",
    "\n",
    "from sonnet.python.modules.base import AbstractModule\n",
    "from sonnet.python.modules.basic import BatchApply, Linear, BatchFlatten\n",
    "from sonnet.python.modules.rnn_core import RNNCore\n",
    "from sonnet.python.modules.gated_rnn import LSTM\n",
    "from sonnet.python.modules.basic_rnn import DeepRNN\n",
    "\n",
    "def _nested_add(nested_a, nested_b):\n",
    "    \"\"\"Add two arbitrarily nested `Tensors`.\"\"\"\n",
    "    return nest.map(lambda a, b: a + b, nested_a, nested_b)\n",
    "\n",
    "\n",
    "def _nested_unary_mul(nested_a, p):\n",
    "    \"\"\"Multiply `Tensors` in arbitrarily nested `Tensor` `nested_a` with `p`.\"\"\"\n",
    "    return nest.map(lambda a: p * a, nested_a)\n",
    "\n",
    "\n",
    "def _nested_zeros_like(nested_a):\n",
    "    return nest.map(tf.zeros_like, nested_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ResidualACTCore(rnn_core.RNNCore):\n",
    "    \"\"\"Adaptive computation time core.\n",
    "\n",
    "    Implementation of the model described in \"Adaptive Computation Time for\n",
    "    Recurrent Neural Networks\" paper, https://arxiv.org/abs/1603.08983.\n",
    "\n",
    "    The `ResidualACTCore` incorporates the pondering RNN of ACT, with different\n",
    "    computation times for each element in the mini batch. Each pondering step is\n",
    "    performed by the `core` passed to the constructor of `ResidualACTCore`.\n",
    "\n",
    "    The output of the `ResidualACTCore` is made of `(act_out, (iteration, remainder)`,\n",
    "    where\n",
    "\n",
    "    * `iteration` counts the number of pondering step in each batch element;\n",
    "    * `remainder` is the remainder as defined in the ACT paper;\n",
    "    * `act_out` is the weighted average output of all pondering steps (see ACT\n",
    "    paper for more info).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, core, output_size, threshold, get_state_for_halting,\n",
    "               name=\"act_core\"):\n",
    "\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "        Args:\n",
    "          core: A `sonnet.RNNCore` object. This should only take a single `Tensor`\n",
    "              in input, and output only a single flat `Tensor`.\n",
    "          output_size: An integer. The size of each output in the sequence.\n",
    "          threshold: A float between 0 and 1. Probability to reach for ACT to stop\n",
    "              pondering.\n",
    "          get_state_for_halting: A callable that can take the `core` state and\n",
    "              return the input to the halting function.\n",
    "          name: A string. The name of this module.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: if `threshold` is not between 0 and 1.\n",
    "          ValueError: if `core` has either nested outputs or outputs that are not\n",
    "              one dimensional.\n",
    "        \"\"\"        \n",
    "        super(ResidualACTCore, self).__init__(name=name)\n",
    "        self._core = core\n",
    "        self._output_size = output_size\n",
    "        self._threshold = threshold\n",
    "        self._get_state_for_halting = get_state_for_halting\n",
    "\n",
    "        if not isinstance(self._core.output_size, tf.TensorShape):\n",
    "            raise ValueError(\"Output of core should be single Tensor.\")\n",
    "        if self._core.output_size.ndims != 1:\n",
    "            raise ValueError(\"Output of core should be 1D.\")\n",
    "        \n",
    "        if not 0 <= self._threshold <= 1:\n",
    "            raise ValueError(\"Threshold should be between 0 and 1, but found {}\".format(self._threshold))    \n",
    "            \n",
    "        \n",
    "    def initial_state(self, *args, **kwargs):\n",
    "        return self._core.initial_state(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return tf.TensorShape([self._output_size]),(tf.TensorShape([1]),tf.TensorShape([1]))\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._core.state_size\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        self._ensure_is_connected()\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        self._ensure_is_connected()\n",
    "        return self._dtype\n",
    "    \n",
    "    \n",
    "    def _cond(self, \n",
    "              unused_x, \n",
    "              unused_cumul_out, \n",
    "              unused_prev_state,\n",
    "              unused_cumul_state, \n",
    "              cumul_halting, \n",
    "              unused_iteration,\n",
    "              unused_remainder):\n",
    "        \"\"\"The `cond` of the `tf.while_loop`.\"\"\"\n",
    "        return tf.reduce_any(cumul_halting < 1)\n",
    "    \n",
    "    \n",
    "    def _body(self, \n",
    "              x, \n",
    "              cumul_out, \n",
    "              prev_state, \n",
    "              cumul_state, \n",
    "              cumul_halting, \n",
    "              iteration, \n",
    "              remainder, \n",
    "              halting_linear, \n",
    "              x_ones):\n",
    "        \n",
    "        \"\"\"The `body` of `tf.while_loop`.\"\"\"\n",
    "        # Increase iteration count only for those elements that are still running.\n",
    "        all_ones = tf.constant(1, shape=(self._batch_size, 1), dtype=self._dtype)\n",
    "        is_iteration_over = tf.equal(cumul_halting, all_ones)\n",
    "        next_iteration = tf.where(is_iteration_over, iteration, iteration + 1)\n",
    "        out, next_state = self._core(x, prev_state)\n",
    "\n",
    "        # 处理不同批次不同停止思考步长问题\n",
    "        prev_controller_state, prev_access_state, prev_read_vectors = prev_state\n",
    "        next_controller_state, next_access_state, next_read_vectors = next_state\n",
    "\n",
    "        next_memory,\\\n",
    "        next_read_weightings,\\\n",
    "        next_write_weightings,\\\n",
    "        next_precedence_weightings,\\\n",
    "        next_link,\\\n",
    "        next_usage = next_access_state\n",
    "    \n",
    "        # Get part of state used to compute halting values.\n",
    "        halting_input = halting_linear(self._get_state_for_halting(next_state))\n",
    "        halting = tf.sigmoid(halting_input, name=\"halting\")\n",
    "        next_cumul_halting_raw = cumul_halting + halting\n",
    "        over_threshold = next_cumul_halting_raw > self._threshold\n",
    "        next_cumul_halting = tf.where(over_threshold, all_ones, next_cumul_halting_raw)\n",
    "        next_remainder = tf.where(over_threshold, remainder, 1 - next_cumul_halting_raw)\n",
    "        p = next_cumul_halting - cumul_halting\n",
    "        p2 = tf.expand_dims(p, axis=2)\n",
    "        p3 = tf.expand_dims(p2, axis=3)\n",
    "        \n",
    "        # 控制器\n",
    "        p_next_controller_state = _nested_unary_mul(next_controller_state, p)\n",
    "        \n",
    "        # 外存储器\n",
    "        p_next_memory = _nested_unary_mul(next_memory, p2)\n",
    "        p_next_read_weightings = _nested_unary_mul(next_read_weightings, p2)\n",
    "        p_next_write_weightings = _nested_unary_mul(next_write_weightings, p2)\n",
    "        p_next_precedence_weightings = _nested_unary_mul(next_precedence_weightings, p2)\n",
    "        p_next_link = _nested_unary_mul(next_link, p3)\n",
    "        p_next_usage = _nested_unary_mul(next_usage, p)\n",
    "        \n",
    "        p_next_access_state = \\\n",
    "        (p_next_memory,\n",
    "         p_next_read_weightings,\n",
    "         p_next_write_weightings,\n",
    "         p_next_precedence_weightings,\n",
    "         p_next_link,\n",
    "         p_next_usage) \n",
    "        \n",
    "        # 外存储器读向量\n",
    "        p_next_read_vectors = _nested_unary_mul(next_read_vectors, p2)\n",
    "        \n",
    "        p_next_state = (p_next_controller_state, p_next_access_state, p_next_read_vectors)\n",
    "        next_cumul_state = _nested_add(cumul_state,p_next_state)\n",
    "    \n",
    "        next_cumul_out = cumul_out + p * out\n",
    "\n",
    "        return (x_ones, \n",
    "                next_cumul_out, \n",
    "                next_state, \n",
    "                next_cumul_state,\n",
    "                next_cumul_halting,\n",
    "                next_iteration, \n",
    "                next_remainder)\n",
    "\n",
    "    \n",
    "    def _build(self, x, prev_state):\n",
    "        \"\"\"Connects the core to the graph.\n",
    "\n",
    "        Args:\n",
    "          x: Input `Tensor` of shape `(batch_size, input_size)`.\n",
    "          prev_state: Previous state. This could be a `Tensor`, or a tuple of\n",
    "              `Tensor`s.\n",
    "\n",
    "        Returns:\n",
    "          The tuple `(output, state)` for this core.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: if the `Tensor` `x` does not have rank 2.\n",
    "        \"\"\"\n",
    "        x.get_shape().with_rank(2)\n",
    "        self._batch_size = x.get_shape().as_list()[0]\n",
    "        self._dtype = x.dtype\n",
    "\n",
    "        x_zeros = tf.concat(\n",
    "            [x, tf.zeros(\n",
    "                shape=(self._batch_size, 1), dtype=self._dtype)], 1)\n",
    "        x_ones = tf.concat(\n",
    "            [x, tf.ones(\n",
    "                shape=(self._batch_size, 1), dtype=self._dtype)], 1)\n",
    "        # Weights for the halting signal\n",
    "        halting_linear = basic.Linear(name=\"halting_linear\", output_size=1)   \n",
    "\n",
    "        body = functools.partial(\n",
    "            self._body, halting_linear=halting_linear, x_ones=x_ones)\n",
    "        cumul_halting_init = tf.zeros(shape=(self._batch_size, 1),\n",
    "                                      dtype=self._dtype)\n",
    "        iteration_init = tf.zeros(shape=(self._batch_size, 1), dtype=self._dtype)\n",
    "        core_output_size = [x.value for x in self._core.output_size]\n",
    "        out_init = tf.zeros(shape=(self._batch_size,) + tuple(core_output_size),\n",
    "                            dtype=self._dtype)\n",
    "        cumul_state_init = _nested_zeros_like(prev_state)\n",
    "        \n",
    "        remainder_init = tf.zeros(shape=(self._batch_size, 1), dtype=self._dtype)\n",
    "\n",
    "        (unused_final_x, \n",
    "         final_out, \n",
    "         unused_final_state, \n",
    "         final_cumul_state,\n",
    "         unused_final_halting, \n",
    "         final_iteration, \n",
    "         final_remainder) = \\\n",
    "        tf.while_loop(\n",
    "            cond= self._cond,                                            \n",
    "            body= body,\n",
    "            loop_vars= [x_zeros, \n",
    "                        out_init, \n",
    "                        prev_state, \n",
    "                        cumul_state_init,\n",
    "                        cumul_halting_init, \n",
    "                        iteration_init, \n",
    "                        remainder_init])\n",
    "        \n",
    "        act_output = basic.Linear(\n",
    "            name=\"act_output_linear\", output_size=self._output_size)(final_out)\n",
    "\n",
    "        return (act_output, (final_iteration, final_remainder)), final_cumul_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ACTCore(rnn_core.RNNCore):\n",
    "    \"\"\"Adaptive computation time core.\n",
    "\n",
    "    Implementation of the model described in \"Adaptive Computation Time for\n",
    "    Recurrent Neural Networks\" paper, https://arxiv.org/abs/1603.08983.\n",
    "\n",
    "    The `ACTCore` incorporates the pondering RNN of ACT, with different\n",
    "    computation times for each element in the mini batch. Each pondering step is\n",
    "    performed by the `core` passed to the constructor of `ACTCore`.\n",
    "\n",
    "    The output of the `ACTCore` is made of `(act_out, (iteration, remainder)`,\n",
    "    where\n",
    "\n",
    "    * `iteration` counts the number of pondering step in each batch element;\n",
    "    * `remainder` is the remainder as defined in the ACT paper;\n",
    "    * `act_out` is the weighted average output of all pondering steps (see ACT\n",
    "    paper for more info).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, core, output_size, threshold, get_state_for_halting,\n",
    "               name=\"act_core\"):\n",
    "\n",
    "        \"\"\"Constructor.\n",
    "\n",
    "        Args:\n",
    "          core: A `sonnet.RNNCore` object. This should only take a single `Tensor`\n",
    "              in input, and output only a single flat `Tensor`.\n",
    "          output_size: An integer. The size of each output in the sequence.\n",
    "          threshold: A float between 0 and 1. Probability to reach for ACT to stop\n",
    "              pondering.\n",
    "          get_state_for_halting: A callable that can take the `core` state and\n",
    "              return the input to the halting function.\n",
    "          name: A string. The name of this module.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: if `threshold` is not between 0 and 1.\n",
    "          ValueError: if `core` has either nested outputs or outputs that are not\n",
    "              one dimensional.\n",
    "        \"\"\"        \n",
    "        super(ACTCore, self).__init__(name=name)\n",
    "        self._core = core\n",
    "        self._output_size = output_size\n",
    "        self._threshold = threshold\n",
    "        self._get_state_for_halting = get_state_for_halting\n",
    "\n",
    "        if not isinstance(self._core.output_size, tf.TensorShape):\n",
    "            raise ValueError(\"Output of core should be single Tensor.\")\n",
    "        if self._core.output_size.ndims != 1:\n",
    "            raise ValueError(\"Output of core should be 1D.\")\n",
    "        \n",
    "        if not 0 <= self._threshold <= 1:\n",
    "            raise ValueError(\"Threshold should be between 0 and 1, but found {}\".format(self._threshold))    \n",
    "            \n",
    "        \n",
    "    def initial_state(self, *args, **kwargs):\n",
    "        return self._core.initial_state(*args, **kwargs)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return tf.TensorShape([self._output_size]),(tf.TensorShape([1]),tf.TensorShape([1]))\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._core.state_size\n",
    "\n",
    "    @property\n",
    "    def batch_size(self):\n",
    "        self._ensure_is_connected()\n",
    "        return self._batch_size\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        self._ensure_is_connected()\n",
    "        return self._dtype\n",
    "    \n",
    "    \n",
    "    def _cond(self, \n",
    "              unused_x, \n",
    "              unused_cumul_out, \n",
    "              unused_prev_state,\n",
    "              unused_cumul_state, \n",
    "              cumul_halting, \n",
    "              unused_iteration,\n",
    "              unused_remainder, \n",
    "              tmp_access_state):\n",
    "        \"\"\"The `cond` of the `tf.while_loop`.\"\"\"\n",
    "        return tf.reduce_any(cumul_halting < 1)\n",
    "    \n",
    "    \n",
    "    def _body(self, \n",
    "              x, \n",
    "              cumul_out, \n",
    "              prev_state, \n",
    "              cumul_state, \n",
    "              cumul_halting, \n",
    "              iteration, \n",
    "              remainder, \n",
    "              tmp_access_state,\n",
    "              halting_linear, \n",
    "              x_ones):\n",
    "        \n",
    "        \"\"\"The `body` of `tf.while_loop`.\"\"\"\n",
    "        # Increase iteration count only for those elements that are still running.\n",
    "        all_ones = tf.constant(1, shape=(self._batch_size, 1), dtype=self._dtype)\n",
    "        is_iteration_over = tf.equal(cumul_halting, all_ones)\n",
    "        next_iteration = tf.where(is_iteration_over, iteration, iteration + 1)\n",
    "        out, next_state = self._core(x, prev_state)\n",
    "\n",
    "        # 处理不同批次不同停止思考步长问题\n",
    "        prev_controller_state, prev_access_state, prev_read_vectors = prev_state\n",
    "        next_controller_state, next_access_state, next_read_vectors = next_state\n",
    "\n",
    "        prev_memory,\\\n",
    "        prev_read_weightings,\\\n",
    "        prev_write_weightings,\\\n",
    "        prev_precedence_weightings,\\\n",
    "        prev_link,\\\n",
    "        prev_usage = prev_access_state\n",
    "\n",
    "        next_memory,\\\n",
    "        next_read_weightings,\\\n",
    "        next_write_weightings,\\\n",
    "        next_precedence_weightings,\\\n",
    "        next_link,\\\n",
    "        next_usage = next_access_state\n",
    "\n",
    "        is_iteration_over = tf.squeeze(is_iteration_over, axis=1)\n",
    "        tmp_memory = tf.where(is_iteration_over, prev_memory, next_memory)\n",
    "        tmp_read_weighting = tf.where(is_iteration_over, prev_read_weightings, next_read_weightings)\n",
    "        tmp_write_weighting = tf.where(is_iteration_over, prev_write_weightings, next_write_weightings)\n",
    "        tmp_precedence_weighting = tf.where(is_iteration_over, prev_precedence_weightings, next_precedence_weightings)\n",
    "        tmp_link = tf.where(is_iteration_over, prev_link, next_link)\n",
    "        tmp_usage = tf.where(is_iteration_over, prev_usage, next_usage)\n",
    "        tmp_access_state = (\n",
    "            tmp_memory, \n",
    "            tmp_read_weighting, \n",
    "            tmp_write_weighting, \n",
    "            tmp_precedence_weighting, \n",
    "            tmp_link, \n",
    "            tmp_usage)\n",
    "    \n",
    "        # Get part of state used to compute halting values.\n",
    "        halting_input = halting_linear(self._get_state_for_halting(next_state))\n",
    "        halting = tf.sigmoid(halting_input, name=\"halting\")\n",
    "        next_cumul_halting_raw = cumul_halting + halting\n",
    "        over_threshold = next_cumul_halting_raw > self._threshold\n",
    "        next_cumul_halting = tf.where(over_threshold, all_ones, next_cumul_halting_raw)\n",
    "        next_remainder = tf.where(over_threshold, remainder, 1 - next_cumul_halting_raw)\n",
    "        p = next_cumul_halting - cumul_halting\n",
    "        p2 = tf.expand_dims(p, axis=2)\n",
    "        p3 = tf.expand_dims(p2, axis=3)\n",
    "        \n",
    "        # 控制器\n",
    "        p_next_controller_state = _nested_unary_mul(next_controller_state, p)\n",
    "        \n",
    "        # 外存储器\n",
    "        p_next_memory = _nested_unary_mul(next_memory, p2)\n",
    "        p_next_read_weightings = _nested_unary_mul(next_read_weightings, p2)\n",
    "        p_next_write_weightings = _nested_unary_mul(next_write_weightings, p2)\n",
    "        p_next_precedence_weightings = _nested_unary_mul(next_precedence_weightings, p2)\n",
    "        p_next_link = _nested_unary_mul(next_link, p3)\n",
    "        p_next_usage = _nested_unary_mul(next_usage, p)\n",
    "        \n",
    "        p_next_access_state = \\\n",
    "        (p_next_memory,\n",
    "         p_next_read_weightings,\n",
    "         p_next_write_weightings,\n",
    "         p_next_precedence_weightings,\n",
    "         p_next_link,\n",
    "         p_next_usage) \n",
    "        \n",
    "        # 外存储器读向量\n",
    "        p_next_read_vectors = _nested_unary_mul(next_read_vectors, p2)\n",
    "        \n",
    "        p_next_state = (p_next_controller_state, p_next_access_state, p_next_read_vectors)\n",
    "        next_cumul_state = _nested_add(cumul_state,p_next_state)\n",
    "    \n",
    "        next_cumul_out = cumul_out + p * out\n",
    "\n",
    "        return (x_ones, \n",
    "                next_cumul_out, \n",
    "                next_state, \n",
    "                next_cumul_state,\n",
    "                next_cumul_halting,\n",
    "                next_iteration, \n",
    "                next_remainder, \n",
    "                tmp_access_state)\n",
    "\n",
    "    \n",
    "    def _build(self, x, prev_state):\n",
    "        \"\"\"Connects the core to the graph.\n",
    "\n",
    "        Args:\n",
    "          x: Input `Tensor` of shape `(batch_size, input_size)`.\n",
    "          prev_state: Previous state. This could be a `Tensor`, or a tuple of\n",
    "              `Tensor`s.\n",
    "\n",
    "        Returns:\n",
    "          The tuple `(output, state)` for this core.\n",
    "\n",
    "        Raises:\n",
    "          ValueError: if the `Tensor` `x` does not have rank 2.\n",
    "        \"\"\"\n",
    "        x.get_shape().with_rank(2)\n",
    "        self._batch_size = x.get_shape().as_list()[0]\n",
    "        self._dtype = x.dtype\n",
    "\n",
    "        x_zeros = tf.concat(\n",
    "            [x, tf.zeros(\n",
    "                shape=(self._batch_size, 1), dtype=self._dtype)], 1)\n",
    "        x_ones = tf.concat(\n",
    "            [x, tf.ones(\n",
    "                shape=(self._batch_size, 1), dtype=self._dtype)], 1)\n",
    "        # Weights for the halting signal\n",
    "        halting_linear = basic.Linear(name=\"halting_linear\", output_size=1)   \n",
    "\n",
    "        body = functools.partial(\n",
    "            self._body, halting_linear=halting_linear, x_ones=x_ones)\n",
    "        cumul_halting_init = tf.zeros(shape=(self._batch_size, 1),\n",
    "                                      dtype=self._dtype)\n",
    "        iteration_init = tf.zeros(shape=(self._batch_size, 1), dtype=self._dtype)\n",
    "        core_output_size = [x.value for x in self._core.output_size]\n",
    "        out_init = tf.zeros(shape=(self._batch_size,) + tuple(core_output_size),\n",
    "                            dtype=self._dtype)\n",
    "        cumul_state_init = _nested_zeros_like(prev_state)\n",
    "        tmp_access_state_init = cumul_state_init[1]\n",
    "        \n",
    "        remainder_init = tf.zeros(shape=(self._batch_size, 1), dtype=self._dtype)\n",
    "\n",
    "        (unused_final_x, \n",
    "         final_out, \n",
    "         unused_final_state, \n",
    "         final_cumul_state,\n",
    "         unused_final_halting, \n",
    "         final_iteration, \n",
    "         final_remainder, \n",
    "         final_access_state) = \\\n",
    "        tf.while_loop(\n",
    "            cond= self._cond,                                            \n",
    "            body= body,\n",
    "            loop_vars= [x_zeros, \n",
    "                        out_init, \n",
    "                        prev_state, \n",
    "                        cumul_state_init,\n",
    "                        cumul_halting_init, \n",
    "                        iteration_init, \n",
    "                        remainder_init, \n",
    "                        tmp_access_state_init])\n",
    "        \n",
    "        act_output = basic.Linear(\n",
    "            name=\"act_output_linear\", output_size=self._output_size)(final_out)\n",
    "    \n",
    "        # 修改，控制器state和读向量使用 pondering 累加权重系数方式，\n",
    "        # 记忆矩阵不使用，记忆矩阵保持展开时间连续性\n",
    "        controller_state, access_state, read_vectors = final_cumul_state\n",
    "        final_cumul_state = (controller_state, final_access_state, read_vectors)\n",
    "\n",
    "        return (act_output, (final_iteration, final_remainder)), final_cumul_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Content-based addressing\n",
    "class calculate_Content_based_addressing(AbstractModule):\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between a query and each word in memory, then\n",
    "    applies a weighted softmax to return a sharp distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                num_heads,\n",
    "                word_size,\n",
    "                epsilon = 1e-6,\n",
    "                name='content_based_addressing'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Initializes the module.\n",
    "\n",
    "        Args:\n",
    "          num_heads: number of memory write heads or read heads.\n",
    "          word_size: memory word size.\n",
    "          name: module name (default 'content_based_addressing')\n",
    "        \"\"\"\n",
    "        super().__init__(name=name) # 调用父类初始化\n",
    "        self._num_heads = num_heads\n",
    "        self._word_size = word_size\n",
    "        self._epsilon = epsilon\n",
    "        \n",
    "        \n",
    "    def _Clip_L2_norm(self, tensor, axis=2):\n",
    "        \"\"\"\n",
    "        计算L2范数，为余弦相似性计算公式分母，这里进行数值平稳化处理\n",
    "        \"\"\"\n",
    "        quadratic_sum = tf.reduce_sum(tf.multiply(tensor, tensor), axis=axis, keep_dims=True)\n",
    "        #return tf.max(tf.sqrt(quadratic_sum + self._epsilon), self._epsilon)           \n",
    "        return tf.sqrt(quadratic_sum + self._epsilon)\n",
    "    \n",
    "    \n",
    "    def _Calculate_cosine_similarity(self, keys, memory):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:      \n",
    "            memory: A 3-D tensor of shape [batch_size, memory_size, word_size]\n",
    "            keys: A 3-D tensor of shape [batch_size, num_heads, word_size]  \n",
    "        Returns:\n",
    "            cosine_similarity: A 3-D tensor of shape `[batch_size, num_heads, memory_size]`.\n",
    "        \"\"\"\n",
    "    \n",
    "        matmul = tf.matmul(keys, memory, adjoint_b=True)\n",
    "        memory_norm = self._Clip_L2_norm(memory, axis=2)\n",
    "        keys_norm = self._Clip_L2_norm(keys, axis=2)\n",
    "        cosine_similarity = matmul / (tf.matmul(keys_norm, memory_norm, adjoint_b=True) + self._epsilon)\n",
    "        return cosine_similarity\n",
    "    \n",
    "    \n",
    "    def _build(self, memory, keys, strengths):\n",
    "        \"\"\"\n",
    "        Connects the CosineWeights module into the graph.\n",
    "\n",
    "        Args:\n",
    "            memory: A 3-D tensor of shape `[batch_size, memory_size, word_size]`.\n",
    "            keys: A 3-D tensor of shape `[batch_size, num_heads, word_size]`.\n",
    "            strengths: A 2-D tensor of shape `[batch_size, num_heads]`.\n",
    "\n",
    "        Returns:\n",
    "            cosine_similarity: A 3-D tensor of shape `[batch_size, num_heads, memory_size]`.\n",
    "            content_weighting: Weights tensor of shape `[batch_size, num_heads, memory_size]`.\n",
    "        \"\"\"\n",
    "        cosine_similarity = self._Calculate_cosine_similarity(keys=keys, memory=memory)\n",
    "        transformed_strengths = tf.expand_dims(strengths, axis=-1)\n",
    "        sharp_activations = cosine_similarity * transformed_strengths\n",
    "        \n",
    "        softmax = BatchApply(module_or_op=tf.nn.softmax)\n",
    "        return softmax(sharp_activations)\n",
    "    \n",
    "#Dynamic_memory_allocation\n",
    "class update_Dynamic_memory_allocation(RNNCore):\n",
    "    \"\"\"\n",
    "    Memory usage that is increased by writing and decreased by reading.\n",
    "\n",
    "    This module is a pseudo-RNNCore whose state is a tensor with values in\n",
    "    the range [0, 1] indicating the usage of each of `memory_size` memory slots.\n",
    "\n",
    "    The usage is:\n",
    "\n",
    "    *   Increased by writing, where usage is increased towards 1 at the write\n",
    "      addresses.\n",
    "    *   Decreased by reading, where usage is decreased after reading from a\n",
    "      location when free_gates is close to 1.\n",
    "    \"\"\"  \n",
    "    \n",
    "    def __init__(self, \n",
    "                 memory_size, \n",
    "                 epsilon = 1e-6,\n",
    "                 name='dynamic_memory_allocation'):\n",
    "        \n",
    "        \"\"\"Creates a module for dynamic memory allocation.\n",
    "\n",
    "        Args:\n",
    "          memory_size: Number of memory slots.\n",
    "          name: Name of the module.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self._memory_size = memory_size\n",
    "        self._epsilon = epsilon\n",
    "        \n",
    "        \n",
    "    def _build(self, \n",
    "               prev_usage, \n",
    "               prev_write_weightings,\n",
    "               free_gates, \n",
    "               prev_read_weightings,\n",
    "               write_gates, \n",
    "               num_writes):\n",
    "        \n",
    "        usage = self._update_usage_vector(prev_usage, \n",
    "                                          prev_write_weightings, \n",
    "                                          free_gates, \n",
    "                                          prev_read_weightings)\n",
    "        \n",
    "        allocation_weightings = \\\n",
    "        self._update_allocation_weightings(usage, \n",
    "                                           write_gates, \n",
    "                                           num_writes)\n",
    "        \n",
    "        return usage, allocation_weightings\n",
    "        \n",
    "\n",
    "    def _update_usage_vector(self, \n",
    "                             prev_usage, \n",
    "                             prev_write_weightings, \n",
    "                             free_gates, \n",
    "                             prev_read_weightings):\n",
    "        \"\"\"\n",
    "        The usage is:\n",
    "\n",
    "        *   Increased by writing, where usage is increased towards 1 at the write\n",
    "          addresses.\n",
    "        *   Decreased by reading, where usage is decreased after reading from a\n",
    "          location when free_gates is close to 1.\n",
    "        \n",
    "        Args:\n",
    "            prev_usage: tensor of shape `[batch_size, memory_size]` giving\n",
    "            usage u_{t - 1} at the previous time step, with entries in range [0, 1].\n",
    "        \n",
    "            prev_write_weightings: tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "            giving write weights at previous time step.\n",
    "            \n",
    "            free_gates: tensor of shape `[batch_size, num_reads]` which indicates\n",
    "            which read heads read memory that can now be freed.\n",
    "          \n",
    "            prev_read_weightings: tensor of shape `[batch_size, num_reads, memory_size]` \n",
    "            giving read weights at previous time step.\n",
    "          \n",
    "        Returns:\n",
    "            usage: tensor of shape `[batch_size, memory_size]` representing updated memory usage.\n",
    "        \"\"\"\n",
    "        prev_write_weightings = tf.stop_gradient(prev_write_weightings)\n",
    "        usage = self._Calculate_usage_vector(prev_usage, prev_write_weightings)\n",
    "        retention = self._Calculate_retention_vector(free_gates, prev_read_weightings)\n",
    "        return usage * retention\n",
    "        \n",
    "        \n",
    "    def _Calculate_usage_vector(self, prev_usage, prev_write_weightings):\n",
    "        \"\"\"\n",
    "        注意这里usage更新使用上一个时间步的数据更新\n",
    "        这个函数是特别添加处理多个写头的,\n",
    "        这个函数计算在写头操作之后记忆矩阵的使用情况usage\n",
    "        \n",
    "        Calcualtes the new usage after writing to memory.\n",
    "\n",
    "        Args:\n",
    "          prev_usage: tensor of shape `[batch_size, memory_size]`.\n",
    "          write_weightings: tensor of shape `[batch_size, num_writes, memory_size]`.\n",
    "\n",
    "        Returns:\n",
    "          New usage, a tensor of shape `[batch_size, memory_size]`.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('usage_after_write'):\n",
    "            # Calculate the aggregated effect of all write heads\n",
    "            fit_prev_write_weightings = 1 - \\\n",
    "            tf.reduce_prod(1 - prev_write_weightings, axis=[1])\n",
    "            \n",
    "            usage_without_free = \\\n",
    "            prev_usage + fit_prev_write_weightings - \\\n",
    "            prev_usage * fit_prev_write_weightings\n",
    "            \n",
    "            return usage_without_free\n",
    "\n",
    "        \n",
    "    def _Calculate_retention_vector(self, free_gates, prev_read_weightings):\n",
    "        \"\"\"\n",
    "        The memory retention vector phi_t represents by how much each location \n",
    "        will not be freed by the gates.\n",
    "        \n",
    "        Args:\n",
    "            free_gates: tensor of shape `[batch_size, num_reads]` with entries in the\n",
    "            range [0, 1] indicating the amount that locations read from can be\n",
    "            freed.\n",
    "            \n",
    "            prev_write_weightings: tensor of shape `[batch_size, num_writes, memory_size]`.\n",
    "        Returns:\n",
    "            retention vector: [batch_size, memory_size]\n",
    "        \"\"\"\n",
    "        with tf.name_scope('usage_after_read'):\n",
    "            free_gates = tf.expand_dims(free_gates, axis=-1)\n",
    "            \n",
    "            retention_vector = tf.reduce_prod(\n",
    "                1 - free_gates * prev_read_weightings, \n",
    "                axis=[1], name='retention')\n",
    "            \n",
    "            return retention_vector     \n",
    "        \n",
    "        \n",
    "    def _update_allocation_weightings(self, usage, write_gates, num_writes):\n",
    "        \"\"\"\n",
    "        Calculates freeness-based locations for writing to.\n",
    "\n",
    "        This finds unused memory by ranking the memory locations by usage, for each\n",
    "        write head. (For more than one write head, we use a \"simulated new usage\"\n",
    "        which takes into account the fact that the previous write head will increase\n",
    "        the usage in that area of the memory.)\n",
    "\n",
    "        Args:\n",
    "            usage: A tensor of shape `[batch_size, memory_size]` representing\n",
    "            current memory usage.\n",
    "\n",
    "            write_gates: A tensor of shape `[batch_size, num_writes]` with values in\n",
    "            the range [0, 1] indicating how much each write head does writing\n",
    "            based on the address returned here (and hence how much usage\n",
    "            increases).\n",
    "\n",
    "            num_writes: The number of write heads to calculate write weights for.\n",
    "\n",
    "        Returns:\n",
    "            tensor of shape `[batch_size, num_writes, memory_size]` containing the\n",
    "            freeness-based write locations. Note that this isn't scaled by `write_gate`; \n",
    "            this scaling must be applied externally.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('update_allocation'):\n",
    "            write_gates = tf.expand_dims(write_gates, axis=-1)\n",
    "            allocation_weightings = []\n",
    "            for i in range(num_writes):\n",
    "                allocation_weightings.append(\n",
    "                    self._Calculate_allocation_weighting(usage))\n",
    "                # update usage to take into account writing to this new allocation\n",
    "                usage += ((1-usage) * write_gates[:,i,:] * allocation_weightings[i])\n",
    "            return tf.stack(allocation_weightings, axis=1)\n",
    "        \n",
    "\n",
    "    def _Calculate_allocation_weighting(self, usage):\n",
    "        \n",
    "        \"\"\"\n",
    "        Computes allocation by sorting `usage`.\n",
    "\n",
    "        This corresponds to the value a = a_t[\\phi_t[j]] in the paper.\n",
    "\n",
    "        Args:\n",
    "              usage: tensor of shape `[batch_size, memory_size]` indicating current\n",
    "              memory usage. This is equal to u_t in the paper when we only have one\n",
    "              write head, but for multiple write heads, one should update the usage\n",
    "              while iterating through the write heads to take into account the\n",
    "              allocation returned by this function.\n",
    "\n",
    "        Returns:\n",
    "          Tensor of shape `[batch_size, memory_size]` corresponding to allocation.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('allocation'):\n",
    "            # Ensure values are not too small prior to cumprod.\n",
    "            usage = self._epsilon + (1 - self._epsilon) * usage\n",
    "            non_usage = 1 - usage\n",
    "            \n",
    "            sorted_non_usage, indices = tf.nn.top_k(\n",
    "            non_usage, k = self._memory_size, name='sort')\n",
    "            \n",
    "            sorted_usage = 1 - sorted_non_usage\n",
    "            prod_sorted_usage = tf.cumprod(sorted_usage, axis=1, exclusive=True)\n",
    "            \n",
    "            sorted_allocation_weighting = sorted_non_usage * prod_sorted_usage\n",
    "            \n",
    "            # This final line \"unsorts\" sorted_allocation, so that the indexing\n",
    "            # corresponds to the original indexing of `usage`.\n",
    "            inverse_indices = self._batch_invert_permutation(indices)\n",
    "            allocation_weighting = self._batch_gather(\n",
    "                sorted_allocation_weighting, inverse_indices)\n",
    "            \n",
    "            return allocation_weighting\n",
    "            \n",
    "\n",
    "    def _batch_invert_permutation(self, permutations):\n",
    "        \n",
    "        \"\"\"\n",
    "        Returns batched `tf.invert_permutation` for every row in `permutations`.\n",
    "        \"\"\"\n",
    "        \n",
    "        with tf.name_scope('batch_invert_permutation', values=[permutations]):\n",
    "            unpacked = tf.unstack(permutations, axis=0)\n",
    "            \n",
    "            inverses = [tf.invert_permutation(permutation) for permutation in unpacked]\n",
    "            return tf.stack(inverses, axis=0)\n",
    "        \n",
    "              \n",
    "    def _batch_gather(self, values, indices):\n",
    "        \"\"\"Returns batched `tf.gather` for every row in the input.\"\"\"\n",
    "        \n",
    "        with tf.name_scope('batch_gather', values=[values, indices]):\n",
    "            unpacked = zip(tf.unstack(values), tf.unstack(indices))\n",
    "            result = [tf.gather(value, index) for value, index in unpacked]\n",
    "            return tf.stack(result)   \n",
    "        \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        pass\n",
    "    \n",
    "#Temporal_memory_linkage\n",
    "class update_Temporal_memory_linkage(RNNCore):\n",
    "    \"\"\"\n",
    "    Keeps track of write order for forward and backward addressing.\n",
    "\n",
    "    This is a pseudo-RNNCore module, whose state is a pair `(link,\n",
    "    precedence_weights)`, where `link` is a (collection of) graphs for (possibly\n",
    "    multiple) write heads (represented by a tensor with values in the range\n",
    "    [0, 1]), and `precedence_weights` records the \"previous write locations\" used\n",
    "    to build the link graphs.\n",
    "\n",
    "    The function `directional_read_weights` computes addresses following the\n",
    "    forward and backward directions in the link graphs.\n",
    "    \"\"\" \n",
    "    def __init__(self, \n",
    "                 memory_size, \n",
    "                 num_writes, \n",
    "                 name='temporal_memory_linkage'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Construct a TemporalLinkage module.\n",
    "\n",
    "        Args:\n",
    "          memory_size: The number of memory slots.\n",
    "          num_writes: The number of write heads.\n",
    "          name: Name of the module.\n",
    "        \"\"\"  \n",
    "        super().__init__(name=name)\n",
    "        self._memory_size = memory_size\n",
    "        self._num_writes = num_writes\n",
    "        \n",
    "        \n",
    "    def _build(self, \n",
    "               prev_link, \n",
    "               prev_precedence_weightings,\n",
    "               prev_read_weightings,\n",
    "               write_weightings):\n",
    "        \"\"\"\n",
    "        Calculate the updated linkage state given the write weights.\n",
    "        \n",
    "        Args:           \n",
    "            prev_links: A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n",
    "            representing the previous link graphs for each write head.\n",
    "\n",
    "            prev_precedence_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "            containing the previous precedence weights.\n",
    "\n",
    "            write_weightings: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "            containing the memory addresses of the different write heads.\n",
    "        \n",
    "        Returns:\n",
    "            link:  A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n",
    "            precedence_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "\n",
    "        \"\"\"\n",
    "        link = self._update_link_matrix(\n",
    "            prev_link, prev_precedence_weightings, write_weightings)\n",
    "        \n",
    "        precedence_weightings = \\\n",
    "        self._update_precedence_weightings(\n",
    "            prev_precedence_weightings, write_weightings)\n",
    "        \n",
    "        forward_weightings = \\\n",
    "        self._Calculate_directional_read_weightings(\n",
    "            link, prev_read_weightings, forward=True)\n",
    "        \n",
    "        backward_weightings = \\\n",
    "        self._Calculate_directional_read_weightings(\n",
    "            link, prev_read_weightings, forward=False)\n",
    "        \n",
    "        return link, precedence_weightings, forward_weightings, backward_weightings  \n",
    "    \n",
    "    \n",
    "    def _update_link_matrix(self, \n",
    "                            prev_link, \n",
    "                            prev_precedence_weightings, \n",
    "                            write_weightings):\n",
    "        \"\"\"\n",
    "        Calculates the new link graphs.\n",
    "\n",
    "        For each write head, the link is a directed graph (represented by a matrix\n",
    "        with entries in range [0, 1]) whose vertices are the memory locations, and\n",
    "        an edge indicates temporal ordering of writes.\n",
    "\n",
    "        Args:\n",
    "          prev_links: A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n",
    "          representing the previous link graphs for each write head.\n",
    "\n",
    "          prev_precedence_weights: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "          which is the previous \"aggregated\" write weights for each write head.\n",
    "\n",
    "          write_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "              containing the new locations in memory written to.\n",
    "\n",
    "        Returns:\n",
    "          A tensor of shape `[batch_size, num_writes, memory_size, memory_size]`\n",
    "          containing the new link graphs for each write head.\n",
    "        \"\"\"    \n",
    "        \n",
    "        with tf.name_scope('link'):\n",
    "                   \n",
    "            write_weightings_i = tf.expand_dims(write_weightings, axis=3)\n",
    "            write_weightings_j = tf.expand_dims(write_weightings, axis=2)\n",
    "            prev_link_scale = 1 - write_weightings_i - write_weightings_j\n",
    "            remove_old_link = prev_link_scale * prev_link\n",
    "            \n",
    "            prev_precedence_weightings_j = tf.expand_dims(\n",
    "                prev_precedence_weightings, axis=2)\n",
    "            add_new_link = write_weightings_i * prev_precedence_weightings_j\n",
    "            \n",
    "            link = remove_old_link + add_new_link\n",
    "            \n",
    "            #Return the link with the diagonal set to zero, to remove self-looping edges.\n",
    "            batch_size = prev_link.get_shape()[0].value\n",
    "            mask = tf.zeros(shape=[batch_size, self._num_writes, self._memory_size], \n",
    "                            dtype=prev_link.dtype)\n",
    "            \n",
    "            fit_link = tf.matrix_set_diag(link, diagonal=mask)\n",
    "            return fit_link\n",
    "        \n",
    "        \n",
    "    def _update_precedence_weightings(self, \n",
    "                                     prev_precedence_weightings, \n",
    "                                     write_weightings):\n",
    "        \"\"\"\n",
    "        Calculates the new precedence weights given the current write weights.\n",
    "\n",
    "        The precedence weights are the \"aggregated write weights\" for each write\n",
    "        head, where write weights with sum close to zero will leave the precedence\n",
    "        weights unchanged, but with sum close to one will replace the precedence\n",
    "        weights.   \n",
    "\n",
    "        Args:\n",
    "          prev_precedence_weightings: A tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "          containing the previous precedence weights.\n",
    "\n",
    "          write_weightings: A tensor of shape `[batch_size, num_writes, memory_size]`\n",
    "          containing the new write weights.\n",
    "\n",
    "        Returns:\n",
    "          A tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "          containing the new precedence weights.  \n",
    "        \"\"\"\n",
    "        with tf.name_scope('precedence_weightings'):\n",
    "            sum_writing = tf.reduce_sum(write_weightings, axis=2, keep_dims=True)\n",
    "            \n",
    "            precedence_weightings = \\\n",
    "            (1 - sum_writing) * prev_precedence_weightings + write_weightings\n",
    "            \n",
    "            return precedence_weightings\n",
    "        \n",
    "\n",
    "    def _Calculate_directional_read_weightings(self,\n",
    "                                               link, \n",
    "                                               prev_read_weightings, \n",
    "                                               forward):\n",
    "        \"\"\"\n",
    "        Calculates the forward or the backward read weightings.\n",
    "\n",
    "        For each read head (at a given address), there are `num_writes` link graphs to follow. \n",
    "        Thus this function computes a read address for each of the\n",
    "        `num_reads * num_writes` pairs of read and write heads.\n",
    "\n",
    "        Args:\n",
    "            link: tensor of shape `[batch_size, num_writes, memory_size, memory_size]`\n",
    "            representing the link graphs L_t.\n",
    "\n",
    "            prev_read_weightsing: tensor of shape `[batch_size, num_reads, memory_size]` \n",
    "            containing the previous read weights w_{t-1}^r.\n",
    "\n",
    "            forward: Boolean indicating whether to follow the \"future\" direction in \n",
    "            the link graph (True) or the \"past\" direction (False).\n",
    "\n",
    "        Returns:\n",
    "            tensor of shape `[batch_size, num_reads, num_writes, memory_size]`\n",
    "\n",
    "            Note: We calculate the forward and backward directions for each pair of\n",
    "            read and write heads; hence we need to tile the read weights and do a\n",
    "            sort of \"outer product\" to get this.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('directional_read_weightings'):\n",
    "            # We calculate the forward and backward directions for each pair of\n",
    "            # read and write heads; hence we need to tile the read weights and do a\n",
    "            # sort of \"outer product\" to get this.\n",
    "            expanded_read_weightings = \\\n",
    "            tf.stack([prev_read_weightings] * self._num_writes, axis=1)\n",
    "            directional_weightings = tf.matmul(expanded_read_weightings, link, adjoint_b=forward)\n",
    "            # Swap dimensions 1, 2 so order is [batch, reads, writes, memory]:\n",
    "            return tf.transpose(directional_weightings, perm=[0,2,1,3])    \n",
    "        \n",
    "# MemoryAccess\n",
    "class MemoryAccess(RNNCore):\n",
    "    \"\"\"\n",
    "    Access module of the Differentiable Neural Computer.\n",
    "\n",
    "    This memory module supports multiple read and write heads. It makes use of:\n",
    "\n",
    "    *   `update_Temporal_memory_linkage` to track the temporal \n",
    "    ordering of writes in memory for each write head.\n",
    "    \n",
    "    *   `update_Dynamic_memory_allocation` for keeping track of \n",
    "    memory usage, where usage increase when a memory location is \n",
    "    written to, and decreases when memory is read from that \n",
    "    the controller says can be freed.\n",
    "      \n",
    "    Write-address selection is done by an interpolation between content-based\n",
    "    lookup and using unused memory.\n",
    "    \n",
    "    Read-address selection is done by an interpolation of content-based lookup\n",
    "    and following the link graph in the forward or backwards read direction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 memory_size = 128, \n",
    "                 word_size = 20, \n",
    "                 num_reads = 1, \n",
    "                 num_writes = 1, \n",
    "                 name='memory_access'):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a MemoryAccess module.\n",
    "\n",
    "        Args:\n",
    "            memory_size: The number of memory slots (N in the DNC paper).\n",
    "            word_size: The width of each memory slot (W in the DNC paper)\n",
    "            num_reads: The number of read heads (R in the DNC paper).\n",
    "            num_writes: The number of write heads (fixed at 1 in the paper).\n",
    "            name: The name of the module.\n",
    "        \"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self._memory_size = memory_size\n",
    "        self._word_size = word_size\n",
    "        self._num_reads = num_reads\n",
    "        self._num_writes = num_writes\n",
    "        \n",
    "        self._write_content_mod = calculate_Content_based_addressing(\n",
    "            num_heads = self._num_writes, \n",
    "            word_size = self._word_size, \n",
    "            name = 'write_content_based_addressing')\n",
    "        \n",
    "        self._read_content_mod = calculate_Content_based_addressing(\n",
    "            num_heads = self._num_reads, \n",
    "            word_size = self._word_size, \n",
    "            name = 'read_content_based_addressing')\n",
    "        \n",
    "        self._temporal_linkage = update_Temporal_memory_linkage(\n",
    "            memory_size = self._memory_size, \n",
    "            num_writes = self._num_writes)\n",
    "        \n",
    "        self._dynamic_allocation = update_Dynamic_memory_allocation(\n",
    "            memory_size = self._memory_size)\n",
    "        \n",
    "        \n",
    "    def _build(self, interface_vector, prev_state):\n",
    "        \"\"\"\n",
    "        Connects the MemoryAccess module into the graph.\n",
    "\n",
    "        Args:\n",
    "            inputs: tensor of shape `[batch_size, input_size]`. \n",
    "            This is used to control this access module.\n",
    "            \n",
    "            prev_state: Instance of `AccessState` containing the previous state.\n",
    "\n",
    "        Returns:\n",
    "            A tuple `(output, next_state)`, where `output` is a tensor of shape\n",
    "            `[batch_size, num_reads, word_size]`, and `next_state` is the new\n",
    "            `AccessState` named tuple at the current time t.\n",
    "        \"\"\"\n",
    "        tape = self._Calculate_interface_parameters(interface_vector)\n",
    "        \n",
    "        prev_memory,\\\n",
    "        prev_read_weightings,\\\n",
    "        prev_write_weightings,\\\n",
    "        prev_precedence_weightings,\\\n",
    "        prev_link,\\\n",
    "        prev_usage = prev_state\n",
    "\n",
    "        # 更新写头\n",
    "        write_weightings,\\\n",
    "        usage = \\\n",
    "        self._update_write_weightings(tape, \n",
    "                                      prev_memory, \n",
    "                                      prev_usage, \n",
    "                                      prev_write_weightings, \n",
    "                                      prev_read_weightings)\n",
    "        \n",
    "        # 更新记忆\n",
    "        memory = self._update_memory(prev_memory, \n",
    "                                     write_weightings, \n",
    "                                     tape['erase_vectors'], \n",
    "                                     tape['write_vectors'])\n",
    "        \n",
    "        # 更新读头\n",
    "        read_weightings,\\\n",
    "        link,\\\n",
    "        precedence_weightings= \\\n",
    "        self._update_read_weightings(tape, \n",
    "                                     memory, \n",
    "                                     write_weightings,\n",
    "                                     prev_read_weightings, \n",
    "                                     prev_precedence_weightings,\n",
    "                                     prev_link)\n",
    "\n",
    "        read_vectors = tf.matmul(read_weightings, memory)\n",
    "        \n",
    "        state = (memory,\n",
    "                 read_weightings,\n",
    "                 write_weightings,\n",
    "                 precedence_weightings,\n",
    "                 link,\n",
    "                 usage)\n",
    "        \n",
    "        return read_vectors, state\n",
    "        \n",
    "\n",
    "    def _update_write_weightings(self, \n",
    "                                  tape, \n",
    "                                  prev_memory, \n",
    "                                  prev_usage, \n",
    "                                  prev_write_weightings, \n",
    "                                  prev_read_weightings):       \n",
    "        \"\"\"\n",
    "        Calculates the memory locations to write to.\n",
    "\n",
    "        This uses a combination of content-based lookup and finding an unused\n",
    "        location in memory, for each write head.\n",
    "\n",
    "        Args:\n",
    "            tape: Collection of inputs to the access module, including controls for\n",
    "            how to chose memory writing, such as the content to look-up and the\n",
    "            weighting between content-based and allocation-based addressing.\n",
    "            \n",
    "            memory: A tensor of shape  `[batch_size, memory_size, word_size]`\n",
    "            containing the current memory contents.\n",
    "            \n",
    "            usage: Current memory usage, which is a tensor of shape \n",
    "            `[batch_size, memory_size]`, used for allocation-based addressing.\n",
    "\n",
    "        Returns:\n",
    "            tensor of shape `[batch_size, num_writes, memory_size]` \n",
    "            indicating where to write to (if anywhere) for each write head.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('update_write_weightings', \\\n",
    "                           values=[tape, prev_memory, prev_usage]):\n",
    "            \n",
    "            write_content_weightings = \\\n",
    "            self._write_content_mod(\n",
    "                prev_memory, \n",
    "                tape['write_content_keys'], \n",
    "                tape['write_content_strengths'])\n",
    "            \n",
    "            usage, write_allocation_weightings = \\\n",
    "            self._dynamic_allocation(\n",
    "                prev_usage, \n",
    "                prev_write_weightings, \n",
    "                tape['free_gates'], \n",
    "                prev_read_weightings, \n",
    "                tape['write_gates'], \n",
    "                self._num_writes)\n",
    "            \n",
    "            allocation_gates = tf.expand_dims(tape['allocation_gates'], axis=-1)\n",
    "            write_gates = tf.expand_dims(tape['write_gates'], axis=-1)\n",
    "            \n",
    "            write_weightings = write_gates * \\\n",
    "            (allocation_gates * write_allocation_weightings + \\\n",
    "             (1 - allocation_gates) * write_content_weightings)\n",
    "            \n",
    "            return write_weightings, usage\n",
    "        \n",
    "        \n",
    "    def _update_memory(self, \n",
    "                       prev_memory, \n",
    "                       write_weightings, \n",
    "                       erase_vectors, \n",
    "                       write_vectors):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            prev_memory: 3-D tensor of shape `[batch_size, memory_size, word_size]`.\n",
    "            write_weightings: 3-D tensor `[batch_size, num_writes, memory_size]`.\n",
    "            erase_vectors: 3-D tensor `[batch_size, num_writes, word_size]`.\n",
    "            write_vectors: 3-D tensor `[batch_size, num_writes, word_size]`.\n",
    "\n",
    "      Returns:\n",
    "            memory: 3-D tensor of shape `[batch_size, num_writes, word_size]`.\n",
    "        \"\"\"\n",
    "        with tf.name_scope('erase_old_memory', \\\n",
    "                           values=[prev_memory, \n",
    "                                   write_weightings, \n",
    "                                   erase_vectors]):\n",
    "\n",
    "            expand_write_weightings = \\\n",
    "            tf.expand_dims(write_weightings, axis=3)\n",
    "\n",
    "            expand_erase_vectors = \\\n",
    "            tf.expand_dims(erase_vectors, axis=2)\n",
    "\n",
    "            # 这里有多个写头，需要使用累成处理多个写头\n",
    "            erase_gates = \\\n",
    "            expand_write_weightings * expand_erase_vectors\n",
    "\n",
    "            retention_gate = \\\n",
    "            tf.reduce_prod(1 - erase_gates, axis=[1])\n",
    "\n",
    "            retention_memory = prev_memory * retention_gate\n",
    "\n",
    "        with tf.name_scope('additive_new_memory', \\\n",
    "                           values=[retention_memory, \n",
    "                                   write_weightings, \n",
    "                                   write_vectors]):\n",
    "\n",
    "            memory = retention_memory + \\\n",
    "            tf.matmul(write_weightings, write_vectors, adjoint_a=True)\n",
    "\n",
    "            return memory\n",
    "        \n",
    "    \n",
    "    def _Calculate_interface_parameters(self, interface_vector):\n",
    "        \"\"\"\n",
    "        Interface parameters. \n",
    "        Before being used to parameterize the memory interactions, \n",
    "        the individual components are then processed with various \n",
    "        functions to ensure that they lie in the correct domain.     \n",
    "        \"\"\"\n",
    "        # read_keys: [batch_size, num_reads, word_size]\n",
    "        read_keys = Linear(\n",
    "            output_size= self._num_reads * self._word_size,\n",
    "            name='read_keys')(interface_vector)\n",
    "        read_keys = tf.reshape(\n",
    "            read_keys, shape=[-1, self._num_reads, self._word_size])\n",
    "\n",
    "        # write_keys: [batch_size, num_writes, word_size]\n",
    "        write_keys = Linear(\n",
    "            output_size= self._num_writes * self._word_size, \n",
    "            name= 'write_keys')(interface_vector)\n",
    "        write_keys = tf.reshape(\n",
    "            write_keys, shape=[-1, self._num_writes, self._word_size])\n",
    "\n",
    "\n",
    "        # read_strengths: [batch_size, num_reads]\n",
    "        read_strengths = Linear(\n",
    "            output_size= self._num_reads,\n",
    "            name= 'read_strengths')(interface_vector)\n",
    "        read_strengths = 1 + tf.nn.softplus(read_strengths)\n",
    "\n",
    "        # write_strengths: [batch_size, num_writes]\n",
    "        write_strengths = Linear(\n",
    "            output_size= self._num_writes,\n",
    "            name='write_strengths')(interface_vector)\n",
    "        write_strengths = 1 + tf.nn.softplus(write_strengths)\n",
    "\n",
    "\n",
    "        # earse_vector: [batch_size, num_writes * word_size]\n",
    "        erase_vectors = Linear(\n",
    "            output_size= self._num_writes * self._word_size,\n",
    "            name='erase_vectors')(interface_vector)\n",
    "        erase_vectors = tf.reshape(\n",
    "            erase_vectors, shape=[-1, self._num_writes, self._word_size])\n",
    "        erase_vectors = tf.nn.sigmoid(erase_vectors)\n",
    "\n",
    "        # write_vectors: [batch_size, num_writes * word_size]\n",
    "        write_vectors = Linear(\n",
    "            output_size= self._num_writes * self._word_size,\n",
    "            name='write_vectors')(interface_vector)\n",
    "        write_vectors = tf.reshape(\n",
    "            write_vectors, shape=[-1, self._num_writes, self._word_size])\n",
    "\n",
    "\n",
    "        # free_gates: [batch_size, num_reads]\n",
    "        free_gates = Linear(\n",
    "            output_size= self._num_reads,\n",
    "            name='free_gates')(interface_vector)\n",
    "        free_gates = tf.nn.sigmoid(free_gates)\n",
    "\n",
    "        # allocation_gates: [batch_size, num_writes]\n",
    "        allocation_gates = Linear(\n",
    "            output_size= self._num_writes,\n",
    "            name='allocation_gates')(interface_vector)\n",
    "        allocation_gates = tf.nn.sigmoid(allocation_gates)\n",
    "\n",
    "        # write_gates: [batch_size, num_writes]\n",
    "        write_gates = Linear(\n",
    "            output_size= self._num_writes,\n",
    "            name='write_gates')(interface_vector)\n",
    "        write_gates = tf.nn.sigmoid(write_gates)\n",
    "\n",
    "        # read_modes: [batch_size, (1 + 2 * num_writes) * num_reads]\n",
    "        num_read_modes = 1 + 2 * self._num_writes\n",
    "        read_modes = Linear(\n",
    "            output_size= self._num_reads * num_read_modes,\n",
    "            name='read_modes')(interface_vector)\n",
    "        read_modes = tf.reshape(\n",
    "            read_modes, shape=[-1, self._num_reads, num_read_modes])\n",
    "        read_modes = BatchApply(tf.nn.softmax)(read_modes)\n",
    "\n",
    "        tape = {\n",
    "            'read_content_keys': read_keys,\n",
    "            'read_content_strengths': read_strengths,\n",
    "            'write_content_keys': write_keys,\n",
    "            'write_content_strengths': write_strengths,\n",
    "            'write_vectors': write_vectors,\n",
    "            'erase_vectors': erase_vectors,\n",
    "            'free_gates': free_gates,\n",
    "            'allocation_gates': allocation_gates,\n",
    "            'write_gates': write_gates,\n",
    "            'read_modes': read_modes,\n",
    "        }\n",
    "        return tape        \n",
    "\n",
    "\n",
    "    def _update_read_weightings(self, \n",
    "                                tape, \n",
    "                                memory, \n",
    "                                write_weightings,\n",
    "                                prev_read_weightings, \n",
    "                                prev_precedence_weightings, \n",
    "                                prev_link):\n",
    "        \"\"\"\n",
    "        Calculates read weights for each read head.\n",
    "\n",
    "        The read weights are a combination of following the link graphs in the\n",
    "        forward or backward directions from the previous read position, and doing\n",
    "        content-based lookup. The interpolation between these different modes is\n",
    "        done by `inputs['read_mode']`.\n",
    "\n",
    "        Args:\n",
    "            inputs: Controls for this access module. \n",
    "            This contains the content-based keys to lookup, \n",
    "            and the weightings for the different read modes.\n",
    "\n",
    "            memory: A tensor of shape `[batch_size, memory_size, word_size]`\n",
    "            containing the current memory contents to do content-based lookup.\n",
    "\n",
    "            prev_read_weights: A tensor of shape `[batch_size, num_reads, memory_size]` \n",
    "            containing the previous read locations.\n",
    "\n",
    "            link: A tensor of shape `[batch_size, num_writes, memory_size, memory_size]` \n",
    "            containing the temporal write transition graphs.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape `[batch_size, num_reads, memory_size]` \n",
    "            containing the read weights for each read head.\n",
    "        \"\"\"    \n",
    "        with tf.name_scope(\n",
    "            'update_read_weightings', \n",
    "            values=[tape, \n",
    "                    memory, \n",
    "                    prev_read_weightings, \n",
    "                    prev_precedence_weightings, \n",
    "                    prev_link]):\n",
    "\n",
    "            read_content_weightings = \\\n",
    "            self._read_content_mod(\n",
    "                memory, \n",
    "                tape['read_content_keys'], \n",
    "                tape['read_content_strengths'])\n",
    "\n",
    "            \n",
    "            link,\\\n",
    "            precedence_weightings,\\\n",
    "            forward_weightings,\\\n",
    "            backward_weightings = \\\n",
    "            self._temporal_linkage(\n",
    "                prev_link, \n",
    "                prev_precedence_weightings, \n",
    "                prev_read_weightings,\n",
    "                write_weightings)\n",
    "            \n",
    "            \n",
    "            backward_mode = tape['read_modes'][:, :, :self._num_writes]\n",
    "            forward_mode = tape['read_modes'][:, :, self._num_writes:2 * self._num_writes]\n",
    "            content_mode = tape['read_modes'][:, :, 2 * self._num_writes]\n",
    "            \n",
    "            backward_ = tf.expand_dims(backward_mode, axis=3) * backward_weightings\n",
    "            backward_ = tf.reduce_sum(backward_, axis=2)\n",
    "            \n",
    "            forward_ = tf.expand_dims(forward_mode, axis=3) * forward_weightings\n",
    "            forward_ = tf.reduce_sum(forward_, axis=2)\n",
    "            \n",
    "            content_ = tf.expand_dims(content_mode, axis=2) * read_content_weightings\n",
    "\n",
    "            read_weightings = backward_ + forward_ + content_\n",
    "\n",
    "            return read_weightings, link, precedence_weightings\n",
    "        \n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        \"\"\"Returns a tuple of the shape of the state tensors.\"\"\"\n",
    "        memory = tf.TensorShape([self._memory_size, self._word_size])\n",
    "        read_weightings = tf.TensorShape([self._num_reads, self._memory_size])\n",
    "        write_weightings = tf.TensorShape([self._num_writes, self._memory_size])\n",
    "        link = tf.TensorShape([self._num_writes, self._memory_size, self._memory_size])\n",
    "        precedence_weightings = tf.TensorShape([self._num_writes, self._memory_size])\n",
    "        usage = tf.TensorShape([self._memory_size])\n",
    "        return (memory, \n",
    "                read_weightings, \n",
    "                write_weightings,\n",
    "                precedence_weightings,\n",
    "                link, \n",
    "                usage)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        \"\"\"\n",
    "        Returns the output shape.\n",
    "        \"\"\"\n",
    "        return tf.TensorShape([self._num_reads, self._word_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNCore_L1(RNNCore):\n",
    "    \"\"\"\n",
    "    DNC core cell\n",
    "    Args:\n",
    "    controller: 控制器\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden_size= 128,\n",
    "                 memory_size= 256, \n",
    "                 word_size= 128, \n",
    "                 num_read_heads= 4, \n",
    "                 num_write_heads= 1,\n",
    "                 name='DNCore'):\n",
    "        \n",
    "        super().__init__(name=name) # 调用父类初始化\n",
    "        with self._enter_variable_scope():\n",
    "            self._controller = LSTM(hidden_size)\n",
    "            self._access = MemoryAccess(\n",
    "                memory_size= memory_size, \n",
    "                word_size= word_size, \n",
    "                num_reads= num_read_heads, \n",
    "                num_writes= num_write_heads)\n",
    "            \n",
    "        self._dnc_output_size = \\\n",
    "        hidden_size + num_read_heads * word_size\n",
    "        \n",
    "        self._num_read_heads = num_read_heads\n",
    "        self._word_size = word_size\n",
    "        \n",
    "        \n",
    "    def _build(self, inputs, prev_tape):\n",
    "        \n",
    "        prev_controller_state,\\\n",
    "        prev_access_state,\\\n",
    "        prev_read_vectors = prev_tape\n",
    "        \n",
    "        batch_flatten = BatchFlatten()\n",
    "        controller_input = tf.concat(\n",
    "            [batch_flatten(inputs), batch_flatten(prev_read_vectors)], axis= 1)\n",
    "        \n",
    "        # 控制器处理数据\n",
    "        controller_output, controller_state = \\\n",
    "        self._controller(controller_input, prev_controller_state)\n",
    "        \n",
    "        # 外存储器交互\n",
    "        read_vectors, access_state = \\\n",
    "        self._access(controller_output, prev_access_state)\n",
    "        \n",
    "        # DNC 输出\n",
    "        dnc_output = tf.concat(\n",
    "            [controller_output, batch_flatten(read_vectors)], axis= 1)\n",
    "        \n",
    "        return dnc_output, (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    def initial_state(self, batch_size, dtype=tf.float32):\n",
    "        controller_state= self._controller.initial_state(batch_size, dtype)\n",
    "        access_state= self._access.initial_state(batch_size, dtype)\n",
    "        read_vectors= tf.zeros([batch_size, self._num_read_heads, self._word_size], dtype=dtype)\n",
    "        return (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        controller_state= self._controller.state_size\n",
    "        access_state= self._access.state_size\n",
    "        read_vectors= tf.TensorShape([self._num_read_heads, self._word_size])\n",
    "        return (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return tf.TensorShape([self._dnc_output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DNCore_L3(RNNCore):\n",
    "    \"\"\"\n",
    "    DNC core cell\n",
    "    Args:\n",
    "    controller: 控制器\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hidden_size= 128,\n",
    "                 memory_size= 256, \n",
    "                 word_size= 128, \n",
    "                 num_read_heads= 3, \n",
    "                 num_write_heads= 1,\n",
    "                 name='DNCore'):\n",
    "        \n",
    "        super().__init__(name=name) # 调用父类初始化\n",
    "        with self._enter_variable_scope():\n",
    "            lstm_1 = LSTM(hidden_size)\n",
    "            lstm_2 = LSTM(hidden_size)\n",
    "            lstm_3 = LSTM(hidden_size)\n",
    "            self._controller = DeepRNN([lstm_1, lstm_2, lstm_3]) \n",
    "            self._access = MemoryAccess(\n",
    "                memory_size= memory_size, \n",
    "                word_size= word_size, \n",
    "                num_reads= num_read_heads, \n",
    "                num_writes= num_write_heads)\n",
    "            \n",
    "        self._dnc_output_size = \\\n",
    "        hidden_size * 3 + num_read_heads * word_size\n",
    "        self._num_read_heads = num_read_heads\n",
    "        self._word_size = word_size\n",
    "        \n",
    "        \n",
    "    def _build(self, inputs, prev_tape):\n",
    "        \n",
    "        prev_controller_state,\\\n",
    "        prev_access_state,\\\n",
    "        prev_read_vectors = prev_tape\n",
    "        \n",
    "        batch_flatten = BatchFlatten()\n",
    "        controller_input = tf.concat(\n",
    "            [batch_flatten(inputs), batch_flatten(prev_read_vectors)], axis= 1)\n",
    "        \n",
    "        # 控制器处理数据\n",
    "        controller_output, controller_state = \\\n",
    "        self._controller(controller_input, prev_controller_state)\n",
    "        \n",
    "        # 外存储器交互\n",
    "        read_vectors, access_state = \\\n",
    "        self._access(controller_output, prev_access_state)\n",
    "        \n",
    "        # DNC 输出\n",
    "        dnc_output = tf.concat(\n",
    "            [controller_output, batch_flatten(read_vectors)], axis= 1)        \n",
    "        return dnc_output, (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    def initial_state(self, batch_size, dtype=tf.float32):\n",
    "        controller_state= self._controller.initial_state(batch_size, dtype)\n",
    "        access_state= self._access.initial_state(batch_size, dtype)\n",
    "        read_vectors= tf.zeros([batch_size, self._num_read_heads, self._word_size], dtype=dtype)\n",
    "        return (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        controller_state= self._controller.state_size\n",
    "        access_state= self._access.state_size\n",
    "        read_vectors= tf.TensorShape([self._num_read_heads, self._word_size])\n",
    "        return (controller_state, access_state, read_vectors)\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return tf.TensorShape([self._dnc_output_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
