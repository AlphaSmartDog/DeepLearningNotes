{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "import datetime\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "import time\n",
    "import os\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义获取数据函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fix_data(path):\n",
    "    tmp = pd.read_csv(path, encoding=\"gbk\", engine='python')\n",
    "    tmp.rename(columns={'Unnamed: 0':'trading_time'}, inplace=True)\n",
    "    tmp['trading_point'] = pd.to_datetime(tmp.trading_time)\n",
    "    del tmp['trading_time']\n",
    "    tmp.set_index(tmp.trading_point, inplace=True)\n",
    "    return tmp\n",
    "\n",
    "def High_2_Low(tmp, freq):\n",
    "    \"\"\"处理从RiceQuant下载的分钟线数据，\n",
    "    从分钟线数据合成低频数据\n",
    "    2017-08-11    \n",
    "    \"\"\"\n",
    "    # 分别处理bar数据\n",
    "    tmp_open = tmp['open'].resample(freq).ohlc()\n",
    "    tmp_open = tmp_open['open'].dropna()\n",
    "\n",
    "    tmp_high = tmp['high'].resample(freq).ohlc()\n",
    "    tmp_high = tmp_high['high'].dropna()\n",
    "\n",
    "    tmp_low = tmp['low'].resample(freq).ohlc()\n",
    "    tmp_low = tmp_low['low'].dropna()\n",
    "\n",
    "    tmp_close = tmp['close'].resample(freq).ohlc()\n",
    "    tmp_close = tmp_close['close'].dropna()\n",
    "\n",
    "    tmp_price = pd.concat([tmp_open, tmp_high, tmp_low, tmp_close], axis=1)\n",
    "    \n",
    "    # 处理成交量\n",
    "    tmp_volume = tmp['volume'].resample(freq).sum()\n",
    "    tmp_volume.dropna(inplace=True)\n",
    "    \n",
    "    return pd.concat([tmp_price, tmp_volume], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 计算技术分析指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import talib \n",
    "\n",
    "def get_factors(index, \n",
    "                Open, \n",
    "                Close, \n",
    "                High, \n",
    "                Low, \n",
    "                Volume,\n",
    "                rolling = 26,\n",
    "                drop=False, \n",
    "                normalization=True):\n",
    "    \n",
    "    tmp = pd.DataFrame()\n",
    "    tmp['tradeTime'] = index\n",
    "    \n",
    "    #累积/派发线（Accumulation / Distribution Line，该指标将每日的成交量通过价格加权累计，\n",
    "    #用以计算成交量的动量。属于趋势型因子\n",
    "    tmp['AD'] = talib.AD(High, Low, Close, Volume)\n",
    "\n",
    "    # 佳庆指标（Chaikin Oscillator），该指标基于AD曲线的指数移动均线而计算得到。属于趋势型因子\n",
    "    tmp['ADOSC'] = talib.ADOSC(High, Low, Close, Volume, fastperiod=3, slowperiod=10)\n",
    "\n",
    "    # 平均动向指数，DMI因子的构成部分。属于趋势型因子\n",
    "    tmp['ADX'] = talib.ADX(High, Low, Close,timeperiod=14)\n",
    "\n",
    "    # 相对平均动向指数，DMI因子的构成部分。属于趋势型因子\n",
    "    tmp['ADXR'] = talib.ADXR(High, Low, Close,timeperiod=14)\n",
    "\n",
    "    # 绝对价格振荡指数\n",
    "    tmp['APO'] = talib.APO(Close, fastperiod=12, slowperiod=26)\n",
    "\n",
    "    # Aroon通过计算自价格达到近期最高值和最低值以来所经过的期间数，帮助投资者预测证券价格从趋势到区域区域或反转的变化，\n",
    "    #Aroon指标分为Aroon、AroonUp和AroonDown3个具体指标。属于趋势型因子\n",
    "    tmp['AROONDown'], tmp['AROONUp'] = talib.AROON(High, Low,timeperiod=14)\n",
    "    tmp['AROONOSC'] = talib.AROONOSC(High, Low,timeperiod=14)\n",
    "\n",
    "    # 均幅指标（Average TRUE Ranger），取一定时间周期内的股价波动幅度的移动平均值，\n",
    "    #是显示市场变化率的指标，主要用于研判买卖时机。属于超买超卖型因子。\n",
    "    tmp['ATR14']= talib.ATR(High, Low, Close, timeperiod=14)\n",
    "    tmp['ATR6']= talib.ATR(High, Low, Close, timeperiod=6)\n",
    "\n",
    "    # 布林带\n",
    "    tmp['Boll_Up'],tmp['Boll_Mid'],tmp['Boll_Down']= talib.BBANDS(Close, timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "\n",
    "    # 均势指标\n",
    "    tmp['BOP'] = talib.BOP(Open, High, Low, Close)\n",
    "\n",
    "    #5日顺势指标（Commodity Channel Index），专门测量股价是否已超出常态分布范围。属于超买超卖型因子。\n",
    "    tmp['CCI5'] = talib.CCI(High, Low, Close, timeperiod=5)\n",
    "    tmp['CCI10'] = talib.CCI(High, Low, Close, timeperiod=10)\n",
    "    tmp['CCI20'] = talib.CCI(High, Low, Close, timeperiod=20)\n",
    "    tmp['CCI88'] = talib.CCI(High, Low, Close, timeperiod=88)\n",
    "\n",
    "    # 钱德动量摆动指标（Chande Momentum Osciliator），与其他动量指标摆动指标如相对强弱指标（RSI）和随机指标（KDJ）不同，\n",
    "    # 钱德动量指标在计算公式的分子中采用上涨日和下跌日的数据。属于超买超卖型因子\n",
    "    tmp['CMO_Close'] = talib.CMO(Close,timeperiod=14)\n",
    "    tmp['CMO_Open'] = talib.CMO(Close,timeperiod=14)\n",
    "\n",
    "    # DEMA双指数移动平均线\n",
    "    tmp['DEMA6'] = talib.DEMA(Close, timeperiod=6)\n",
    "    tmp['DEMA12'] = talib.DEMA(Close, timeperiod=12)\n",
    "    tmp['DEMA26'] = talib.DEMA(Close, timeperiod=26)\n",
    "\n",
    "    # DX 动向指数\n",
    "    tmp['DX'] = talib.DX(High, Low, Close,timeperiod=14)\n",
    "\n",
    "    # EMA 指数移动平均线\n",
    "    tmp['EMA6'] = talib.EMA(Close, timeperiod=6)\n",
    "    tmp['EMA12'] = talib.EMA(Close, timeperiod=12)\n",
    "    tmp['EMA26'] = talib.EMA(Close, timeperiod=26)\n",
    "\n",
    "    # KAMA 适应性移动平均线\n",
    "    tmp['KAMA'] = talib.KAMA(Close, timeperiod=30)\n",
    "\n",
    "    # MACD\n",
    "    tmp['MACD_DIF'],tmp['MACD_DEA'],tmp['MACD_bar'] = talib.MACD(Close, fastperiod=12, slowperiod=24, signalperiod=9)\n",
    "\n",
    "    # 中位数价格 不知道是什么意思\n",
    "    tmp['MEDPRICE'] = talib.MEDPRICE(High, Low)\n",
    "\n",
    "    # 负向指标 负向运动\n",
    "    tmp['MiNUS_DI'] = talib.MINUS_DI(High, Low, Close,timeperiod=14)\n",
    "    tmp['MiNUS_DM'] = talib.MINUS_DM(High, Low,timeperiod=14)\n",
    "\n",
    "    # 动量指标（Momentom Index），动量指数以分析股价波动的速度为目的，研究股价在波动过程中各种加速，\n",
    "    #减速，惯性作用以及股价由静到动或由动转静的现象。属于趋势型因子\n",
    "    tmp['MOM'] = talib.MOM(Close, timeperiod=10)\n",
    "\n",
    "    # 归一化平均值范围\n",
    "    tmp['NATR'] = talib.NATR(High, Low, Close,timeperiod=14)\n",
    "\n",
    "    # OBV \t能量潮指标（On Balance Volume，OBV），以股市的成交量变化来衡量股市的推动力，\n",
    "    #从而研判股价的走势。属于成交量型因子\n",
    "    tmp['OBV'] = talib.OBV(Close, Volume)\n",
    "\n",
    "    # PLUS_DI 更向指示器\n",
    "    tmp['PLUS_DI'] = talib.PLUS_DI(High, Low, Close,timeperiod=14)\n",
    "    tmp['PLUS_DM'] = talib.PLUS_DM(High, Low, timeperiod=14)\n",
    "\n",
    "    # PPO 价格振荡百分比\n",
    "    tmp['PPO'] = talib.PPO(Close, fastperiod=6, slowperiod= 26, matype=0)\n",
    "\n",
    "    # ROC 6日变动速率（Price Rate of Change），以当日的收盘价和N天前的收盘价比较，\n",
    "    #通过计算股价某一段时间内收盘价变动的比例，应用价格的移动比较来测量价位动量。属于超买超卖型因子。\n",
    "    tmp['ROC6'] = talib.ROC(Close, timeperiod=6)\n",
    "    tmp['ROC20'] = talib.ROC(Close, timeperiod=20)\n",
    "    #12日量变动速率指标（Volume Rate of Change），以今天的成交量和N天前的成交量比较，\n",
    "    #通过计算某一段时间内成交量变动的幅度，应用成交量的移动比较来测量成交量运动趋向，\n",
    "    #达到事先探测成交量供需的强弱，进而分析成交量的发展趋势及其将来是否有转势的意愿，\n",
    "    #属于成交量的反趋向指标。属于成交量型因子\n",
    "    tmp['VROC6'] = talib.ROC(Volume, timeperiod=6)\n",
    "    tmp['VROC20'] = talib.ROC(Volume, timeperiod=20)\n",
    "\n",
    "    # ROC 6日变动速率（Price Rate of Change），以当日的收盘价和N天前的收盘价比较，\n",
    "    #通过计算股价某一段时间内收盘价变动的比例，应用价格的移动比较来测量价位动量。属于超买超卖型因子。\n",
    "    tmp['ROCP6'] = talib.ROCP(Close, timeperiod=6)\n",
    "    tmp['ROCP20'] = talib.ROCP(Close, timeperiod=20)\n",
    "    #12日量变动速率指标（Volume Rate of Change），以今天的成交量和N天前的成交量比较，\n",
    "    #通过计算某一段时间内成交量变动的幅度，应用成交量的移动比较来测量成交量运动趋向，\n",
    "    #达到事先探测成交量供需的强弱，进而分析成交量的发展趋势及其将来是否有转势的意愿，\n",
    "    #属于成交量的反趋向指标。属于成交量型因子\n",
    "    tmp['VROCP6'] = talib.ROCP(Volume, timeperiod=6)\n",
    "    tmp['VROCP20'] = talib.ROCP(Volume, timeperiod=20)\n",
    "\n",
    "    # RSI\n",
    "    tmp['RSI'] = talib.RSI(Close, timeperiod=14)\n",
    "\n",
    "    # SAR 抛物线转向\n",
    "    tmp['SAR'] = talib.SAR(High, Low, acceleration=0.02, maximum=0.2)\n",
    "\n",
    "    # TEMA\n",
    "    tmp['TEMA6'] = talib.TEMA(Close, timeperiod=6)\n",
    "    tmp['TEMA12'] = talib.TEMA(Close, timeperiod=12)\n",
    "    tmp['TEMA26'] = talib.TEMA(Close, timeperiod=26)\n",
    "\n",
    "    # TRANGE 真实范围\n",
    "    tmp['TRANGE'] = talib.TRANGE(High, Low, Close)\n",
    "\n",
    "    # TYPPRICE 典型价格\n",
    "    tmp['TYPPRICE'] = talib.TYPPRICE(High, Low, Close)\n",
    "\n",
    "    # TSF 时间序列预测\n",
    "    tmp['TSF'] = talib.TSF(Close, timeperiod=14)\n",
    "\n",
    "    # ULTOSC 极限振子\n",
    "    tmp['ULTOSC'] = talib.ULTOSC(High, Low, Close, timeperiod1=7, timeperiod2=14, timeperiod3=28)\n",
    "\n",
    "    # 威廉指标\n",
    "    tmp['WILLR'] = talib.WILLR(High, Low, Close, timeperiod=14)\n",
    "    \n",
    "    # 标准化\n",
    "    if normalization:\n",
    "        factors_list = tmp.columns.tolist()[1:]\n",
    "\n",
    "        if rolling >= 26:\n",
    "            for i in factors_list:\n",
    "                tmp[i] = (tmp[i] - tmp[i].rolling(window=rolling, center=False).mean())\\\n",
    "                /tmp[i].rolling(window=rolling, center=False).std()\n",
    "        elif rolling < 26 & rolling > 0:\n",
    "            print ('Recommended rolling range greater than 26')\n",
    "        elif rolling <=0:\n",
    "            for i in factors_list:\n",
    "                tmp[i] = (tmp[i] - tmp[i].mean())/tmp[i].std()\n",
    "            \n",
    "    if drop:\n",
    "        tmp.dropna(inplace=True)\n",
    "        \n",
    "    tmp.set_index('tradeTime', inplace=True)\n",
    "    \n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 处理数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tmp = fix_data('期货测试数据/白银88.csv')\n",
    "\n",
    "# targets 1d 数据合成\n",
    "tmp_1d = High_2_Low(tmp, '1d')\n",
    "rolling = 88\n",
    "targets = tmp_1d\n",
    "targets['returns'] =  targets['close'].shift(-2) / targets['close'] - 1.0\n",
    "targets['upper_boundary']= targets.returns.rolling(rolling).mean() + 0.5 * targets.returns.rolling(rolling).std()\n",
    "targets['lower_boundary']= targets.returns.rolling(rolling).mean() - 0.5 * targets.returns.rolling(rolling).std()\n",
    "targets.dropna(inplace=True)\n",
    "targets['labels'] = 1\n",
    "targets.loc[targets['returns']>=targets['upper_boundary'], 'labels'] = 2\n",
    "targets.loc[targets['returns']<=targets['lower_boundary'], 'labels'] = 0\n",
    "\n",
    "# factors 1d 数据合成\n",
    "tmp_1d = High_2_Low(tmp, '1d')\n",
    "Index = tmp_1d.index\n",
    "High = tmp_1d.high.values\n",
    "Low = tmp_1d.low.values\n",
    "Close = tmp_1d.close.values\n",
    "Open = tmp_1d.open.values\n",
    "Volume = tmp_1d.volume.values\n",
    "factors = get_factors(Index, Open, Close, High, Low, Volume, rolling = 26, drop=True)\n",
    "\n",
    "factors = factors.loc[:targets.index[-1]]\n",
    "\n",
    "tmp_factors_1 = factors.iloc[:12]\n",
    "targets = targets.loc[tmp_factors_1.index[-1]:]\n",
    "\n",
    "gather_list = np.arange(factors.shape[0])[11:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 转换数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = np.array(factors).reshape(-1, 1, factors.shape[1])\n",
    "\n",
    "def dense_to_one_hot(labels_dense):\n",
    "    \"\"\"标签 转换one hot 编码\n",
    "    输入labels_dense 必须为非负数\n",
    "    2016-11-21\n",
    "    \"\"\"\n",
    "    num_classes = len(np.unique(labels_dense)) # np.unique 去掉重复函数\n",
    "    raws_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(raws_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((raws_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot  \n",
    "\n",
    "targets = dense_to_one_hot(targets['labels'])\n",
    "targets = np.expand_dims(targets, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from DNCore import DNCoreDeepLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier_DNCoreDeepLSTM(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 inputs, \n",
    "                 targets,\n",
    "                 gather_list=None,\n",
    "                 batch_size=1, \n",
    "                 hidden_size=20, \n",
    "                 memory_size=20, \n",
    "                 num_reads=3,\n",
    "                 num_writes=1,  \n",
    "                 learning_rate = 1e-3,\n",
    "                 optimizer_epsilon = 1e-8,\n",
    "                 l2_coefficient = 1e-3,\n",
    "                 max_gard_norm = 50,\n",
    "                 reset_graph = True):\n",
    "        \n",
    "        if reset_graph:\n",
    "            tf.reset_default_graph()\n",
    "        # 控制参数\n",
    "        self._tmp_inputs = inputs\n",
    "        self._tmp_targets = targets\n",
    "        self._in_length = None\n",
    "        self._in_width = inputs.shape[2]\n",
    "        self._out_length = None\n",
    "        self._out_width = targets.shape[2]\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # 声明会话\n",
    "        self._sess = tf.InteractiveSession()\n",
    "        \n",
    "        self._inputs = tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[self._in_length, self._batch_size, self._in_width],\n",
    "            name='inputs')\n",
    "        self._targets = tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[self._out_length, self._batch_size, self._out_width],\n",
    "            name='targets')\n",
    "        \n",
    "        self._RNNCoreCell = DNCoreDeepLSTM(\n",
    "            dnc_output_size=self._out_width, \n",
    "            hidden_size=hidden_size, \n",
    "            memory_size=memory_size, \n",
    "            word_size=self._in_width, \n",
    "            num_read_heads=num_reads,\n",
    "            num_write_heads=num_writes)\n",
    "        \n",
    "        self._initial_state = \\\n",
    "        self._RNNCoreCell.initial_state(batch_size)\n",
    "        \n",
    "        output_sequences, _ = \\\n",
    "        tf.nn.dynamic_rnn(cell= self._RNNCoreCell, \n",
    "                          inputs=self._inputs, \n",
    "                          initial_state=self._initial_state, \n",
    "                          time_major=True)\n",
    "        \n",
    "        self._original_output_sequences = output_sequences\n",
    "        if gather_list is not None:\n",
    "            output_sequences = tf.gather(output_sequences, gather_list)\n",
    "        \n",
    "        # L2 正则化测试 2017-09-03 \n",
    "        self._trainable_variables = tf.trainable_variables()\n",
    "        _l2_regularizer = tf.add_n([tf.nn.l2_loss(v) for v in self._trainable_variables])        \n",
    "        self._l2_regularizer = _l2_regularizer * l2_coefficient / len(self._trainable_variables)\n",
    "        \n",
    "        rnn_cost = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=self._targets, logits=output_sequences)\n",
    "        self._rnn_cost = tf.reduce_mean(rnn_cost)\n",
    "        self._cost = self._rnn_cost + self._l2_regularizer\n",
    "        \n",
    "        \n",
    "        train_pred = tf.nn.softmax(output_sequences, dim=2)\n",
    "        correct_pred = tf.equal(tf.argmax(train_pred,2), tf.argmax(self._targets,2))\n",
    "        self._accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        # Set up optimizer with global norm clipping.\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self._cost, trainable_variables), max_gard_norm)\n",
    "        global_step = tf.get_variable(\n",
    "            name=\"global_step\",\n",
    "            shape=[],\n",
    "            dtype=tf.int64,\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n",
    "        \n",
    "        optimizer = tf.contrib.opt.NadamOptimizer(\n",
    "            learning_rate=learning_rate, epsilon=optimizer_epsilon)\n",
    "        self._train_step = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables), global_step=global_step)  \n",
    "        \n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "        self._variables_saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "    def fit(self, \n",
    "            training_iters =1e2,             \n",
    "            display_step = 5, \n",
    "            save_path = None,\n",
    "            restore_path = None):\n",
    "        \n",
    "        if restore_path is not None:\n",
    "            self._variables_saver.restore(self._sess, restore_path)\n",
    "              \n",
    "        for scope in range(np.int(training_iters)):\n",
    "            self._sess.run([self._train_step],\n",
    "                           feed_dict = {self._inputs:self._tmp_inputs, self._targets:self._tmp_targets})\n",
    "            \n",
    "            if scope % display_step == 0:\n",
    "                loss, acc, l2_loss, rnn_loss = self._sess.run(\n",
    "                    [self._cost, self._accuracy, self._l2_regularizer, self._rnn_cost], \n",
    "                    feed_dict = {self._inputs:self._tmp_inputs, self._targets:self._tmp_targets}) \n",
    "                print (scope, '  loss--', loss, '  acc--', acc, '  l2_loss', l2_loss, '  rnn_cost', rnn_loss)                       \n",
    "                    \n",
    "        print (\"Optimization Finished!\")         \n",
    "        loss, acc, l2_loss, rnn_loss = self._sess.run(\n",
    "            [self._cost, self._accuracy, self._l2_regularizer, self._rnn_cost], \n",
    "            feed_dict = {self._inputs:self._tmp_inputs, self._targets:self._tmp_targets}) \n",
    "        print ('Model assessment  loss--', loss, '  acc--', acc, '  l2_loss', l2_loss, '  rnn_cost', rnn_loss)      \n",
    "        # 保存模型可训练变量\n",
    "        if save_path is not None:\n",
    "            self._variables_saver.save(self._sess, save_path) \n",
    "            \n",
    "    def close(self):\n",
    "        self._sess.close()\n",
    "        print ('结束进程，清理tensorflow内存/显存占用')\n",
    "        \n",
    "    def pred(self, inputs, gather_list=None, restore_path=None):\n",
    "        output_sequences = self._original_output_sequences\n",
    "        if gather_list is not None:\n",
    "            output_sequences = tf.gather(output_sequences, gather_list)\n",
    "        probability = tf.nn.softmax(output_sequences)\n",
    "        classification = tf.argmax(probability, axis=-1)\n",
    "        return self._sess.run([probability, classification],feed_dict = {self._inputs:inputs})\n",
    "    \n",
    "    def restore_trainable_variables(self, restore_path):\n",
    "        self._variables_saver.restore(self._sess, restore_path)\n",
    "    \n",
    "    def score(self, inputs, targets, gather_list=None):\n",
    "        acc = self._sess.run(\n",
    "            self._accuracy,\n",
    "            feed_dict = {self._inputs:self._tmp_inputs, \n",
    "                         self._targets:self._tmp_targets})\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The `skip_connections` argument will be deprecated. Please use snt.SkipConnectionCore instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\PythonDevelopment\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   loss-- 1.09564   acc-- 0.520918   l2_loss 0.0654176   rnn_cost 1.03022\n",
      "5   loss-- 1.02197   acc-- 0.535762   l2_loss 0.0611229   rnn_cost 0.960843\n",
      "10   loss-- 0.972609   acc-- 0.569501   l2_loss 0.0578166   rnn_cost 0.914792\n",
      "15   loss-- 0.987256   acc-- 0.554656   l2_loss 0.0553882   rnn_cost 0.931868\n",
      "20   loss-- 0.896166   acc-- 0.618084   l2_loss 0.0538287   rnn_cost 0.842338\n",
      "25   loss-- 0.85353   acc-- 0.654521   l2_loss 0.0525933   rnn_cost 0.800937\n",
      "30   loss-- 0.80255   acc-- 0.678812   l2_loss 0.051608   rnn_cost 0.750942\n",
      "35   loss-- 0.816485   acc-- 0.665317   l2_loss 0.050841   rnn_cost 0.765644\n",
      "40   loss-- 0.743006   acc-- 0.696356   l2_loss 0.050242   rnn_cost 0.692764\n",
      "45   loss-- 0.693751   acc-- 0.736842   l2_loss 0.0498432   rnn_cost 0.643907\n",
      "50   loss-- 0.66089   acc-- 0.740891   l2_loss 0.0495279   rnn_cost 0.611363\n",
      "55   loss-- 0.61985   acc-- 0.766532   l2_loss 0.0493084   rnn_cost 0.570541\n",
      "60   loss-- 0.586818   acc-- 0.780027   l2_loss 0.0491726   rnn_cost 0.537645\n",
      "65   loss-- 0.545643   acc-- 0.801619   l2_loss 0.0490661   rnn_cost 0.496577\n",
      "70   loss-- 0.508457   acc-- 0.82861   l2_loss 0.0490282   rnn_cost 0.459428\n",
      "75   loss-- 0.564138   acc-- 0.778677   l2_loss 0.0490356   rnn_cost 0.515102\n",
      "80   loss-- 0.459515   acc-- 0.847503   l2_loss 0.0490646   rnn_cost 0.41045\n",
      "85   loss-- 0.427858   acc-- 0.869096   l2_loss 0.0491549   rnn_cost 0.378703\n",
      "90   loss-- 0.404269   acc-- 0.874494   l2_loss 0.04922   rnn_cost 0.355049\n",
      "95   loss-- 0.428364   acc-- 0.848853   l2_loss 0.0493303   rnn_cost 0.379033\n",
      "100   loss-- 0.35319   acc-- 0.897436   l2_loss 0.0494398   rnn_cost 0.30375\n",
      "105   loss-- 0.325448   acc-- 0.916329   l2_loss 0.0495598   rnn_cost 0.275888\n",
      "110   loss-- 0.335813   acc-- 0.904184   l2_loss 0.0496586   rnn_cost 0.286155\n",
      "115   loss-- 0.295673   acc-- 0.917679   l2_loss 0.0497647   rnn_cost 0.245909\n",
      "120   loss-- 0.270637   acc-- 0.933873   l2_loss 0.0498683   rnn_cost 0.220769\n",
      "125   loss-- 0.41744   acc-- 0.854251   l2_loss 0.0499807   rnn_cost 0.367459\n",
      "130   loss-- 0.260258   acc-- 0.944669   l2_loss 0.0501238   rnn_cost 0.210134\n",
      "135   loss-- 0.230941   acc-- 0.955466   l2_loss 0.0502792   rnn_cost 0.180662\n",
      "140   loss-- 0.206087   acc-- 0.962213   l2_loss 0.0504352   rnn_cost 0.155652\n",
      "145   loss-- 0.249695   acc-- 0.939271   l2_loss 0.0505658   rnn_cost 0.199129\n",
      "150   loss-- 0.188839   acc-- 0.967611   l2_loss 0.0506697   rnn_cost 0.138169\n",
      "155   loss-- 0.164535   acc-- 0.973009   l2_loss 0.050785   rnn_cost 0.11375\n",
      "160   loss-- 0.259585   acc-- 0.924426   l2_loss 0.0508611   rnn_cost 0.208724\n",
      "165   loss-- 0.157975   acc-- 0.977058   l2_loss 0.0509473   rnn_cost 0.107027\n",
      "170   loss-- 0.137439   acc-- 0.982456   l2_loss 0.0510269   rnn_cost 0.0864121\n",
      "175   loss-- 0.200951   acc-- 0.951417   l2_loss 0.0510403   rnn_cost 0.149911\n",
      "180   loss-- 0.133147   acc-- 0.989204   l2_loss 0.0510718   rnn_cost 0.0820754\n",
      "185   loss-- 0.114043   acc-- 0.993252   l2_loss 0.0511048   rnn_cost 0.0629386\n",
      "190   loss-- 0.101665   acc-- 0.987854   l2_loss 0.0511078   rnn_cost 0.050557\n",
      "195   loss-- 0.0907295   acc-- 0.994602   l2_loss 0.0510639   rnn_cost 0.0396656\n",
      "Optimization Finished!\n",
      "Model assessment  loss-- 0.114459   acc-- 0.979757   l2_loss 0.0510009   rnn_cost 0.0634583\n",
      "0.979757\n",
      "结束进程，清理tensorflow内存/显存占用\n"
     ]
    }
   ],
   "source": [
    "a = Classifier_DNCoreDeepLSTM(\n",
    "    inputs, \n",
    "    targets, \n",
    "    gather_list,\n",
    "    hidden_size=50, \n",
    "    memory_size=50, \n",
    "    learning_rate = 1e-3, \n",
    "    l2_coefficient=5e-3)\n",
    "\n",
    "a.fit(training_iters = 200,\n",
    "      display_step=5, \n",
    "      save_path='models/DNCoreDeepLSTM_NADM_saver_1.ckpt')\n",
    "\n",
    "print (a.score(inputs, targets, gather_list))\n",
    "a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
