{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import TAF\n",
    "import datetime\n",
    "import talib \n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "factors = pd.read_csv('HS300_15m.csv')\n",
    "\n",
    "index = factors['index']\n",
    "High = factors.high.values\n",
    "Low = factors.low.values\n",
    "Close = factors.close.values\n",
    "Open = factors.open.values\n",
    "Volume = factors.volume.values\n",
    "\n",
    "factors = TAF.get_factors(index, Open, Close, High, Low, Volume, drop=True)\n",
    "\n",
    "factors = factors.iloc[-700 * 16 - 11 * 16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始时间 2014-02-25\n",
      "结束时间 2016-12-30\n"
     ]
    }
   ],
   "source": [
    "start_date = factors.index[11*16][:10]\n",
    "end_date = factors.index[-1][:10]\n",
    "\n",
    "print ('开始时间', start_date)\n",
    "print ('结束时间', end_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rolling = 88\n",
    "\n",
    "targets = pd.read_csv('HS300_1d.csv')\n",
    "targets.rename(columns={'Unnamed: 0':'tradeDate'}, inplace=True)\n",
    "targets['returns'] = targets.close.shift(-5)/ targets.close - 1.\n",
    "targets['labels'] = 1\n",
    "targets['upper_boundary']= targets.returns.rolling(rolling).mean() + 0.5 * targets.returns.rolling(rolling).std()\n",
    "targets['lower_boundary']= targets.returns.rolling(rolling).mean() - 0.5 * targets.returns.rolling(rolling).std()\n",
    "\n",
    "targets.dropna(inplace=True)\n",
    "targets.loc[targets['returns']>=targets['upper_boundary'], 'labels'] = 2\n",
    "targets.loc[targets['returns']<=targets['lower_boundary'], 'labels'] = 0\n",
    "\n",
    "targets.set_index('tradeDate', inplace=True)\n",
    "targets= targets.loc[start_date:end_date, 'labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 输入数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs = np.array(factors).reshape(-1, 1, 58)\n",
    "\n",
    "def dense_to_one_hot(labels_dense):\n",
    "    \"\"\"标签 转换one hot 编码\n",
    "    输入labels_dense 必须为非负数\n",
    "    2016-11-21\n",
    "    \"\"\"\n",
    "    num_classes = len(np.unique(labels_dense)) # np.unique 去掉重复函数\n",
    "    raws_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(raws_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((raws_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot  \n",
    "\n",
    "targets = dense_to_one_hot(targets)\n",
    "targets = np.expand_dims(targets, axis=1)\n",
    "\n",
    "train_inputs = inputs[:-100*16]\n",
    "test_inputs = inputs[-100*16 - 11 * 16:]\n",
    "\n",
    "train_targets = targets[:-100]\n",
    "test_targets = targets[-100:]\n",
    "\n",
    "train_gather_list = np.arange(train_inputs.shape[0])\n",
    "train_gather_list = train_gather_list.reshape([-1,16])[11:]\n",
    "train_gather_list = train_gather_list[:,-1]\n",
    "\n",
    "test_gather_list = np.arange(test_inputs.shape[0])\n",
    "test_gather_list = test_gather_list.reshape([-1,16])[11:]\n",
    "test_gather_list = test_gather_list[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DNCoreLSTM 分类器测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from DNCore import DNCoreLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier_DNCoreLSTM(object):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 inputs, \n",
    "                 targets,\n",
    "                 gather_list=None,\n",
    "                 batch_size=1, \n",
    "                 hidden_size=10, \n",
    "                 memory_size=10, \n",
    "                 num_reads=3,\n",
    "                 num_writes=1,  \n",
    "                 learning_rate = 1e-4,\n",
    "                 optimizer_epsilon = 1e-10,\n",
    "                 l2_coefficient = 1e-3,\n",
    "                 max_gard_norm = 50,\n",
    "                 reset_graph = True):\n",
    "        \n",
    "        if reset_graph:\n",
    "            tf.reset_default_graph()\n",
    "        # 控制参数\n",
    "        self._tmp_inputs = inputs\n",
    "        self._tmp_targets = targets\n",
    "        self._in_length = None\n",
    "        self._in_width = inputs.shape[2]\n",
    "        self._out_length = None\n",
    "        self._out_width = targets.shape[2]\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        # 声明会话\n",
    "        self._sess = tf.InteractiveSession()\n",
    "        \n",
    "        self._inputs = tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[self._in_length, self._batch_size, self._in_width],\n",
    "            name='inputs')\n",
    "        self._targets = tf.placeholder(\n",
    "            dtype=tf.float32,\n",
    "            shape=[self._out_length, self._batch_size, self._out_width],\n",
    "            name='targets')\n",
    "        \n",
    "        self._RNNCoreCell = DNCoreLSTM(\n",
    "            dnc_output_size=self._out_width, \n",
    "            hidden_size=hidden_size, \n",
    "            memory_size=memory_size, \n",
    "            word_size=self._in_width, \n",
    "            num_read_heads=num_reads,\n",
    "            num_write_heads=num_writes)\n",
    "        \n",
    "        self._initial_state = \\\n",
    "        self._RNNCoreCell.initial_state(batch_size)\n",
    "        \n",
    "        output_sequences, _ = \\\n",
    "        tf.nn.dynamic_rnn(cell= self._RNNCoreCell, \n",
    "                          inputs=self._inputs, \n",
    "                          initial_state=self._initial_state, \n",
    "                          time_major=True)\n",
    "        \n",
    "        self._original_output_sequences = output_sequences\n",
    "        if gather_list is not None:\n",
    "            output_sequences = tf.gather(output_sequences, gather_list)\n",
    "        \n",
    "        # L2 正则化测试 2017-09-04 \n",
    "        self._trainable_variables = tf.trainable_variables()\n",
    "        _l2_regularizer = tf.add_n([tf.nn.l2_loss(v) for v in self._trainable_variables])        \n",
    "        self._l2_regularizer = _l2_regularizer * l2_coefficient / len(self._trainable_variables)\n",
    "        \n",
    "        rnn_cost = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels=self._targets, logits=output_sequences)\n",
    "        self._rnn_cost = tf.reduce_mean(rnn_cost)\n",
    "        \n",
    "        self._cost = self._rnn_cost + self._l2_regularizer\n",
    "        \n",
    "        \n",
    "        train_pred = tf.nn.softmax(output_sequences, dim=2)\n",
    "        correct_pred = tf.equal(tf.argmax(train_pred,2), tf.argmax(self._targets,2))\n",
    "        self._accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        \n",
    "        # Set up optimizer with global norm clipping.\n",
    "        trainable_variables = tf.trainable_variables()\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(self._cost, trainable_variables), max_gard_norm)\n",
    "        global_step = tf.get_variable(\n",
    "            name=\"global_step\",\n",
    "            shape=[],\n",
    "            dtype=tf.int64,\n",
    "            initializer=tf.zeros_initializer(),\n",
    "            trainable=False,\n",
    "            collections=[tf.GraphKeys.GLOBAL_VARIABLES, tf.GraphKeys.GLOBAL_STEP])\n",
    "        \n",
    "        optimizer = tf.train.RMSPropOptimizer(\n",
    "            learning_rate=learning_rate, epsilon=optimizer_epsilon)\n",
    "        self._train_step = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables), global_step=global_step)  \n",
    "        \n",
    "        self._sess.run(tf.global_variables_initializer())\n",
    "        self._variables_saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "    def fit(self, \n",
    "            training_iters =1e2,             \n",
    "            display_step = 5, \n",
    "            save_path = None,\n",
    "            restore_path = None):\n",
    "        \n",
    "        if restore_path is not None:\n",
    "            self._variables_saver.restore(self._sess, restore_path)\n",
    "              \n",
    "        for scope in range(np.int(training_iters)):\n",
    "            self._sess.run([self._train_step],\n",
    "                           feed_dict = {self._inputs:self._tmp_inputs, self._targets:self._tmp_targets})\n",
    "            \n",
    "            if scope % display_step == 0:\n",
    "                loss, acc, l2_loss, rnn_loss = self._sess.run(\n",
    "                    [self._cost, self._accuracy, self._l2_regularizer, self._rnn_cost], \n",
    "                    feed_dict = {self._inputs:self._tmp_inputs, self._targets:self._tmp_targets}) \n",
    "                print (scope, '  loss--', loss, '  acc--', acc, 'l2_loss', l2_loss, 'rnn_cost', rnn_loss)                       \n",
    "                    \n",
    "        print (\"Optimization Finished!\")         \n",
    "        loss, acc, l2_loss, rnn_loss = self._sess.run(\n",
    "            [self._cost, self._accuracy, self._l2_regularizer, self._rnn_cost], \n",
    "            feed_dict = {self._inputs:self._tmp_inputs, self._targets:self._tmp_targets}) \n",
    "        print ('Model assessment  loss--', loss, '  acc--', acc, 'l2_loss', l2_loss, 'rnn_cost', rnn_loss)     \n",
    "        # 保存模型可训练变量\n",
    "        if save_path is not None:\n",
    "            self._variables_saver.save(self._sess, save_path) \n",
    "            \n",
    "    def close(self):\n",
    "        self._sess.close()\n",
    "        print ('结束进程，清理tensorflow内存/显存占用')\n",
    "        \n",
    "    def pred(self, inputs, gather_list=None, restore_path=None):\n",
    "        output_sequences = self._original_output_sequences\n",
    "        if gather_list is not None:\n",
    "            output_sequences = tf.gather(output_sequences, gather_list)\n",
    "        probability = tf.nn.softmax(output_sequences)\n",
    "        classification = tf.argmax(probability, axis=-1)\n",
    "        return self._sess.run([probability, classification],feed_dict = {self._inputs:inputs})\n",
    "    \n",
    "    def restore_trainable_variables(self, restore_path):\n",
    "        self._variables_saver.restore(self._sess, restore_path)\n",
    "    \n",
    "    def score(self, inputs, targets, gather_list=None):\n",
    "        acc = self._sess.run(\n",
    "            self._accuracy,\n",
    "            feed_dict = {self._inputs:self._tmp_inputs, \n",
    "                         self._targets:self._tmp_targets})\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\PythonDevelopment\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gradients_impl.py:95: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   loss-- 1.10531   acc-- 0.316667 l2_loss 0.00677044 rnn_cost 1.09854\n",
      "Optimization Finished!\n",
      "Model assessment  loss-- 1.10529   acc-- 0.313333 l2_loss 0.00677044 rnn_cost 1.09852\n"
     ]
    }
   ],
   "source": [
    "a = Classifier_DNCoreLSTM(train_inputs, train_targets, train_gather_list)\n",
    "a.fit(training_iters = 5)\n",
    "#a.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31333333"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.score(train_inputs, train_targets, train_gather_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-30d9529ff51c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstop\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
